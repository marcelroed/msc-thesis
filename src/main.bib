
@video{alexanderaminiMITS19120192019,
  title = {{{MIT}} 6.{{S191}} (2019): {{Biologically Inspired Neural Networks}} ({{IBM}})},
  shorttitle = {{{MIT}} 6.{{S191}} (2019)},
  editor = {{Alexander Amini}},
  date = {2019-02-19},
  url = {https://www.youtube.com/watch?v=4lY-oAY0aQU},
  urldate = {2022-04-19},
  abstract = {MIT Introduction to Deep Learning 6.S191: Lecture 8 A Biologically Plausible Learning Algorithm for Neural Networks Lecturer: Dmitry Krotov MIT/IBM Watson AI Lab Guest Lecture January 2019 For all lectures, slides and lab materials: http://introtodeeplearning.com},
  editortype = {director}
}

@misc{alonBottleneckGraphNeural2021,
  title = {On the {{Bottleneck}} of {{Graph Neural Networks}} and Its {{Practical Implications}}},
  author = {Alon, Uri and Yahav, Eran},
  date = {2021-03-09},
  number = {arXiv:2006.05205},
  eprint = {2006.05205},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.05205},
  url = {http://arxiv.org/abs/2006.05205},
  urldate = {2022-09-22},
  abstract = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/4MX436YY/Alon and Yahav - 2021 - On the Bottleneck of Graph Neural Networks and its.pdf;/Users/marcel/Zotero/storage/QJ4HKJ5P/2006.html}
}

@article{baldiTheoryLocalLearning2016,
  title = {A Theory of Local Learning, the Learning Channel, and the Optimality of Backpropagation},
  author = {Baldi, Pierre and Sadowski, Peter},
  date = {2016-11},
  journaltitle = {Neural Networks: The Official Journal of the International Neural Network Society},
  shortjournal = {Neural Netw},
  volume = {83},
  eprint = {27584574},
  eprinttype = {pmid},
  pages = {51--74},
  issn = {1879-2782},
  doi = {10.1016/j.neunet.2016.07.006},
  abstract = {In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. For any learning algorithm, the capacity of the learning channel can be defined as the number of bits provided about the error gradient per weight, divided by the number of required operations per weight. We estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost. This result is also shown to be true for recurrent networks, by unfolding them in time. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far.},
  langid = {english},
  keywords = {Algorithms,Backpropagation,Deep learning,Feedback,Hebbian learning,Learning channel,Machine Learning,Neural Networks; Computer,Supervised learning,Unsupervised learning},
  file = {/Users/marcel/Zotero/storage/EW4Y3XDC/Baldi and Sadowski - 2016 - A theory of local learning, the learning channel, .pdf}
}

@unpublished{bekkersBSplineCNNsLie2021,
  title = {B-{{Spline CNNs}} on {{Lie Groups}}},
  author = {Bekkers, Erik J.},
  date = {2021-03-22},
  eprint = {1909.12057},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.12057},
  urldate = {2021-07-27},
  abstract = {Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides in which rotation equivariance plays a key role and facial landmark localization in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/S8N43RV5/Bekkers_2021_B-Spline CNNs on Lie Groups.pdf;/Users/marcel/Zotero/storage/T67R44TL/1909.html}
}

@unpublished{bengioBiologicallyPlausibleDeep2016,
  title = {Towards {{Biologically Plausible Deep Learning}}},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  date = {2016-08-08},
  eprint = {1502.04156},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1502.04156},
  urldate = {2022-04-19},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/P55GBFZP/Bengio et al. - 2016 - Towards Biologically Plausible Deep Learning.pdf;/Users/marcel/Zotero/storage/3YUEVRNL/1502.html}
}

@unpublished{bodnarNeuralSheafDiffusion2022,
  title = {Neural {{Sheaf Diffusion}}: {{A Topological Perspective}} on {{Heterophily}} and {{Oversmoothing}} in {{GNNs}}},
  shorttitle = {Neural {{Sheaf Diffusion}}},
  author = {Bodnar, Cristian and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Liò, Pietro and Bronstein, Michael M.},
  date = {2022-02-09},
  eprint = {2202.04579},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/2202.04579},
  urldate = {2022-03-17},
  abstract = {Cellular sheaves equip graphs with "geometrical" structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain state-of-the-art results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Algebraic Topology},
  file = {/Users/marcel/Zotero/storage/246E963V/Bodnar et al. - 2022 - Neural Sheaf Diffusion A Topological Perspective .pdf;/Users/marcel/Zotero/storage/YA2S6Y9W/2202.html}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-05-02},
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13478},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2022-09-22},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/HKQ8WGWC/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/marcel/Zotero/storage/GI3THZXN/2104.html}
}

@online{bronsteinOversquashingBottlenecksGraph2021,
  title = {Over-Squashing, {{Bottlenecks}}, and {{Graph Ricci}} Curvature},
  author = {Bronstein, Michael},
  date = {2021-11-30T16:32:40},
  url = {https://towardsdatascience.com/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16},
  urldate = {2022-05-13},
  abstract = {A concept from differential geometry called Ricci curvature allows to understand the phenomena of over-squashing and bottlenecks in GNNs},
  langid = {english},
  organization = {{Medium}}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-09-22},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/marcel/Zotero/storage/A3S76FE7/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/marcel/Zotero/storage/GTIWYHFS/2005.html}
}

@article{buzsakiLogdynamicBrainHow2014,
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  shorttitle = {The Log-Dynamic Brain},
  author = {Buzsáki, György and Mizuseki, Kenji},
  date = {2014-04},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {15},
  number = {4},
  pages = {264--278},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3687},
  url = {https://www.nature.com/articles/nrn3687},
  urldate = {2022-05-02},
  abstract = {At many physiological and anatomical levels in the brain, the distribution of numerous parameters is strongly skewed with a heavy tail and typically follows a lognormal distribution.The power and frequency relationship of brain oscillations is typically expressed in a log scale.Network synchrony, measured as a fraction of spiking neurons in a given time window, shows lognormal distribution in all brain states.Firing rates, spike bursts and synaptic weights follow a lognormal distribution. Importantly, these parameters remain correlated across brain states, environments and situations.The log-dynamic patterns of networks may be supported by the lognormal distribution of corticocortical connections strengths and axon diameters.A preconfigured, strongly connected minority of fast-firing neurons form the backbone of brain connectivity and serve as an ever-ready, fast-acting system. However, full performance of the brain also depends on the activity of very large numbers of weakly connected and slow-firing majority of neurons.},
  issue = {4},
  langid = {english},
  keywords = {Cellular neuroscience,Spine regulation and structure,Synaptic transmission},
  file = {/Users/marcel/Zotero/storage/IVNQQPKR/Buzsáki and Mizuseki - 2014 - The log-dynamic brain how skewed distributions af.pdf;/Users/marcel/Zotero/storage/BGXXSWFN/nrn3687.html}
}

@incollection{carterPositiveNegativeLiberty2022,
  title = {Positive and {{Negative Liberty}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Carter, Ian},
  editor = {Zalta, Edward N.},
  date = {2022},
  edition = {Spring 2022},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  url = {https://plato.stanford.edu/archives/spr2022/entries/liberty-positive-negative/},
  urldate = {2022-04-10},
  abstract = {Negative liberty is the absence of obstacles, barriers or constraints.One has negative liberty to the extent that actions are available toone in this negative sense. Positive liberty is the possibility ofacting — or the fact of acting — in such a way as to takecontrol of one’s life and realize one’s fundamentalpurposes. While negative liberty is usually attributed to individualagents, positive liberty is sometimes attributed to collectivities, orto individuals considered primarily as members of givencollectivities., The idea of distinguishing between a negative and a positive sense ofthe term ‘liberty’ goes back at least to Kant, and wasexamined and defended in depth by Isaiah Berlin in the 1950s and’60s. Discussions about positive and negative liberty normallytake place within the context of political and social philosophy. Theyare distinct from, though sometimes related to, philosophicaldiscussions about free will. Work on the nature of positive liberty often overlaps, however, withwork on the nature of autonomy., As Berlin showed, negative and positive liberty are not merely twodistinct kinds of liberty; they can be seen as rival, incompatibleinterpretations of a single political ideal. Since few people claim tobe against liberty, the way this term is interpreted and defined canhave important political implications. Political liberalism tends to presuppose a negative definition of liberty: liberalsgenerally claim that if one favors individual liberty one should placestrong limitations on the activities of the state. Critics ofliberalism often contest this implication by contesting the negativedefinition of liberty: they argue that the pursuit of libertyunderstood as self-realization or as self-determination (whether ofthe individual or of the collectivity) can require state interventionof a kind not normally allowed by liberals., Many authors prefer to talk of positive and negative freedom.This is only a difference of style, and the terms‘liberty’ and ‘freedom’ are normally usedinterchangeably by political and social philosophers. Although someattempts have been made to distinguish between liberty and freedom(Pitkin 1988; Williams 2001; Dworkin 2011), generally speaking thesehave not caught on. Neither can they be translated into other Europeanlanguages, which contain only the one term, of either Latin orGermanic origin (e.g. liberté, Freiheit), where Englishcontains both.},
  keywords = {abilities,action,autonomy: in moral and political philosophy,autonomy: personal,Berlin; Isaiah,civil rights,coercion,free will,freedom: of speech,legal rights,liberalism,libertarianism,limits of law,paternalism,republicanism,rights,rights: human},
  file = {/Users/marcel/Zotero/storage/23WIHMMC/liberty-positive-negative.html}
}

@misc{chamberlainBeltramiFlowNeural2021a,
  title = {Beltrami {{Flow}} and {{Neural Diffusion}} on {{Graphs}}},
  author = {Chamberlain, Benjamin Paul and Rowbottom, James and Eynard, Davide and Di Giovanni, Francesco and Dong, Xiaowen and Bronstein, Michael M.},
  date = {2021-10-18},
  number = {arXiv:2110.09443},
  eprint = {2110.09443},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.09443},
  url = {http://arxiv.org/abs/2110.09443},
  urldate = {2022-07-04},
  abstract = {We propose a novel class of graph neural networks based on the discretised Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural networks and achieves state-of-the-art results on several benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/SYW7Q4MD/Chamberlain et al. - 2021 - Beltrami Flow and Neural Diffusion on Graphs.pdf;/Users/marcel/Zotero/storage/QGQP35M9/2110.html}
}

@unpublished{chamberlainGRANDGraphNeural2021,
  title = {{{GRAND}}: {{Graph Neural Diffusion}}},
  shorttitle = {{{GRAND}}},
  author = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria and Webb, Stefan and Rossi, Emanuele and Bronstein, Michael M.},
  date = {2021-09-22},
  eprint = {2106.10934},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2106.10934},
  urldate = {2022-04-24},
  abstract = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/GMWRQIGK/Chamberlain et al. - 2021 - GRAND Graph Neural Diffusion.pdf;/Users/marcel/Zotero/storage/84TLAZQ9/2106.html}
}

@unpublished{chenEquivariantPointNetwork2021,
  title = {Equivariant {{Point Network}} for {{3D Point Cloud Analysis}}},
  author = {Chen, Haiwei and Liu, Shichen and Chen, Weikai and Li, Hao},
  date = {2021-04-02},
  eprint = {2103.14147},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.14147},
  urldate = {2021-07-27},
  abstract = {Features that are equivariant to a larger group of symmetries have been shown to be more discriminative and powerful in recent studies. However, higher-order equivariant features often come with an exponentially-growing computational cost. Furthermore, it remains relatively less explored how rotation-equivariant features can be leveraged to tackle 3D shape alignment tasks. While many past approaches have been based on either non-equivariant or invariant descriptors to align 3D shapes, we argue that such tasks may benefit greatly from an equivariant framework. In this paper, we propose an effective and practical SE(3) (3D translation and rotation) equivariant network for point cloud analysis that addresses both problems. First, we present SE(3) separable point convolution, a novel framework that breaks down the 6D convolution into two separable convolutional operators alternatively performed in the 3D Euclidean and SO(3) spaces. This significantly reduces the computational cost without compromising the performance. Second, we introduce an attention layer to effectively harness the expressiveness of the equivariant features. While jointly trained with the network, the attention layer implicitly derives the intrinsic local frame in the feature space and generates attention vectors that can be integrated into different alignment tasks. We evaluate our approach through extensive studies and visual interpretations. The empirical results demonstrate that our proposed model outperforms strong baselines in a variety of benchmarks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/7A7Q65WK/Chen et al_2021_Equivariant Point Network for 3D Point Cloud Analysis.pdf;/Users/marcel/Zotero/storage/AK39UPC4/2103.html}
}

@misc{chenMeasuringRelievingOversmoothing2019,
  title = {Measuring and {{Relieving}} the {{Over-smoothing Problem}} for {{Graph Neural Networks}} from the {{Topological View}}},
  author = {Chen, Deli and Lin, Yankai and Li, Wei and Li, Peng and Zhou, Jie and Sun, Xu},
  date = {2019-11-18},
  number = {arXiv:1909.03211},
  eprint = {1909.03211},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1909.03211},
  urldate = {2022-09-22},
  abstract = {Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective;(2) AdaGraph which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/TSMR7SDY/Chen et al. - 2019 - Measuring and Relieving the Over-smoothing Problem.pdf;/Users/marcel/Zotero/storage/C9U58B4K/1909.html}
}

@inproceedings{chenNeuralOrdinaryDifferential2018,
  title = {Neural {{Ordinary Differential Equations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html},
  urldate = {2022-09-12},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  file = {/Users/marcel/Zotero/storage/AX2QCDBI/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf}
}

@unpublished{cmscollaborationDeepNeuralNetwork2020,
  title = {A Deep Neural Network for Simultaneous Estimation of b Jet Energy and Resolution},
  author = {CMS Collaboration},
  date = {2020-11-05},
  eprint = {1912.06046},
  eprinttype = {arxiv},
  primaryclass = {hep-ex, physics:physics},
  doi = {10.1007/s41781-020-00041-z},
  url = {http://arxiv.org/abs/1912.06046},
  urldate = {2021-09-27},
  abstract = {We describe a method to obtain point and dispersion estimates for the energies of jets arising from b quarks produced in proton-proton collisions at an energy of \$\textbackslash sqrt\{s\} =\$ 13 TeV at the CERN LHC. The algorithm is trained on a large simulated sample of b jets and validated on data recorded by the CMS detector in 2017 corresponding to an integrated luminosity of 41 fb\$\^\{-1\}\$. A multivariate regression algorithm based on a deep feed-forward neural network employs jet composition and shape information, and the properties of reconstructed secondary vertices associated with the jet. The results of the algorithm are used to improve the sensitivity of analyses that make use of b jets in the final state, such as the observation of Higgs boson decay to \$\textbackslash mathrm\{b\textbackslash bar\{b\}\}\$.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Experiment,Physics - Data Analysis; Statistics and Probability},
  file = {/Users/marcel/Zotero/storage/77R3WMC5/CMS Collaboration_2020_A deep neural network for simultaneous estimation of b jet energy and resolution.pdf;/Users/marcel/Zotero/storage/TF3QH7CG/1912.html}
}

@article{cmscollaborationMissingTransverseEnergy2011,
  title = {Missing Transverse Energy Performance of the {{CMS}} Detector},
  author = {CMS Collaboration},
  date = {2011-09-09},
  journaltitle = {Journal of Instrumentation},
  shortjournal = {J. Inst.},
  volume = {6},
  number = {09},
  eprint = {1106.5048},
  eprinttype = {arxiv},
  pages = {P09001-P09001},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/6/09/P09001},
  url = {http://arxiv.org/abs/1106.5048},
  urldate = {2021-07-27},
  abstract = {During 2010 the LHC delivered pp collisions with a centre-of-mass energy of 7 TeV. In this paper, the results of comprehensive studies of missing transverse energy as measured by the CMS detector are presented. The results cover the measurements of the scale and resolution for missing transverse energy, and the effects of multiple pp interactions within the same bunch crossings on the scale and resolution. Anomalous measurements of missing transverse energy are studied, and algorithms for their identification are described. The performances of several reconstruction algorithms for calculating missing transverse energy are compared. An algorithm, called missing-transverse-energy significance, which estimates the compatibility of the reconstructed missing transverse energy with zero, is described, and its performance is demonstrated.},
  archiveprefix = {arXiv},
  keywords = {Physics - Instrumentation and Detectors},
  file = {/Users/marcel/Zotero/storage/LAFE6MHY/CMS Collaboration_2011_Missing transverse energy performance of the CMS detector.pdf;/Users/marcel/Zotero/storage/GK7BEWQV/1106.html}
}

@unpublished{cohenIntertwinersInducedRepresentations2018,
  title = {Intertwiners between {{Induced Representations}} (with {{Applications}} to the {{Theory}} of {{Equivariant Neural Networks}})},
  author = {Cohen, Taco S. and Geiger, Mario and Weiler, Maurice},
  date = {2018-03-30},
  eprint = {1803.10743},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.10743},
  urldate = {2021-07-27},
  abstract = {Group equivariant and steerable convolutional neural networks (regular and steerable G-CNNs) have recently emerged as a very effective model class for learning from signal data such as 2D and 3D images, video, and other data where symmetries are present. In geometrical terms, regular G-CNNs represent data in terms of scalar fields ("feature channels"), whereas the steerable G-CNN can also use vector or tensor fields ("capsules") to represent data. In algebraic terms, the feature spaces in regular G-CNNs transform according to a regular representation of the group G, whereas the feature spaces in Steerable G-CNNs transform according to the more general induced representations of G. In order to make the network equivariant, each layer in a G-CNN is required to intertwine between the induced representations associated with its input and output space. In this paper we present a general mathematical framework for G-CNNs on homogeneous spaces like Euclidean space or the sphere. We show, using elementary methods, that the layers of an equivariant network are convolutional if and only if the input and output feature spaces transform according to an induced representation. This result, which follows from G.W. Mackey's abstract theory on induced representations, establishes G-CNNs as a universal class of equivariant network architectures, and generalizes the important recent work of Kondor \& Trivedi on the intertwiners between regular representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/83LSNKQD/Cohen et al_2018_Intertwiners between Induced Representations (with Applications to the Theory.pdf;/Users/marcel/Zotero/storage/3SNH6WY9/1803.html}
}

@unpublished{cohenSteerableCNNs2016,
  title = {Steerable {{CNNs}}},
  author = {Cohen, Taco S. and Welling, Max},
  date = {2016-12-26},
  eprint = {1612.08498},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1612.08498},
  urldate = {2021-08-13},
  abstract = {It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/WYH9D7P6/Cohen_Welling_2016_Steerable CNNs.pdf;/Users/marcel/Zotero/storage/745Z5NL7/1612.html}
}

@article{cragoExposingMemoryAccess2018,
  title = {Exposing {{Memory Access Patterns}} to {{Improve Instruction}} and {{Memory Efficiency}} in {{GPUs}}},
  author = {Crago, Neal C. and Stephenson, Mark and Keckler, Stephen W.},
  date = {2018-10-29},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  volume = {15},
  number = {4},
  pages = {45:1--45:23},
  issn = {1544-3566},
  doi = {10.1145/3280851},
  url = {https://doi.org/10.1145/3280851},
  urldate = {2022-09-24},
  abstract = {Modern computing workloads often have high memory intensity, requiring high bandwidth access to memory. The memory request patterns of these workloads vary and include regular strided accesses and indirect (pointer-based) accesses. Such applications require a large number of address generation instructions and a high degree of memory-level parallelism. This article proposes new memory instructions that exploit strided and indirect memory request patterns and improve efficiency in GPU architectures. The new instructions reduce address calculation instructions by offloading addressing to dedicated hardware, and reduce destructive memory request interference by grouping related requests together. Our results show that we can eliminate 33\% of dynamic instructions across 16 GPU benchmarks. These improvements result in an overall runtime improvement of 26\%, an energy reduction of 18\%, and a reduction in energy-delay product of 32\%.},
  keywords = {GPU architecture,vector instruction sets,vector memory instructions},
  file = {/Users/marcel/Zotero/storage/GWFZS5P2/Crago et al. - 2018 - Exposing Memory Access Patterns to Improve Instruc.pdf}
}

@article{crickRecentExcitementNeural1989,
  title = {The Recent Excitement about Neural Networks},
  author = {Crick, Francis},
  date = {1989-01},
  journaltitle = {Nature},
  volume = {337},
  number = {6203},
  pages = {129--132},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/337129a0},
  url = {https://www.nature.com/articles/337129a0},
  urldate = {2022-05-01},
  abstract = {The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects.},
  issue = {6203},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/marcel/Zotero/storage/EJS4WCCJ/Crick - 1989 - The recent excitement about neural networks.pdf;/Users/marcel/Zotero/storage/N82DTA9X/337129a0.html}
}

@article{dalgatyBioInspiredArchitecturesSubstantially2021,
  title = {Bio-{{Inspired Architectures Substantially Reduce}} the {{Memory Requirements}} of {{Neural Network Models}}},
  author = {Dalgaty, Thomas and Miller, John P. and Vianello, Elisa and Casas, Jérôme},
  date = {2021},
  journaltitle = {Frontiers in Neuroscience},
  volume = {15},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2021.612359},
  urldate = {2022-05-02},
  abstract = {We propose a neural network model for the jumping escape response behavior observed in the cricket cercal sensory system. This sensory system processes low-intensity air currents in the animal's immediate environment generated by predators, competitors, and mates. Our model is inspired by decades of physiological and anatomical studies. We compare the performance of our model with a model derived through a universal approximation, or a generic deep learning, approach, and demonstrate that, to achieve the same performance, these models required between one and two orders of magnitude more parameters. Furthermore, since the architecture of the bio-inspired model is defined by a set of logical relations between neurons, we find that the model is open to interpretation and can be understood. This work demonstrates the potential of incorporating bio-inspired architectural motifs, which have evolved in animal nervous systems, into memory efficient neural network models.},
  file = {/Users/marcel/Zotero/storage/KWWTKY23/Dalgaty et al. - 2021 - Bio-Inspired Architectures Substantially Reduce th.pdf}
}

@article{dealmeidaMemoryRetrievalTime2007,
  title = {Memory Retrieval Time and Memory Capacity of the {{CA3}} Network: {{Role}} of Gamma Frequency Oscillations},
  shorttitle = {Memory Retrieval Time and Memory Capacity of the {{CA3}} Network},
  author = {de Almeida, Licurgo and Idiart, Marco and Lisman, John E.},
  options = {useprefix=true},
  date = {2007-11},
  journaltitle = {Learning \& Memory},
  shortjournal = {Learn Mem},
  volume = {14},
  number = {11},
  eprint = {18007022},
  eprinttype = {pmid},
  pages = {795--806},
  issn = {1072-0502},
  doi = {10.1101/lm.730207},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2080581/},
  urldate = {2022-05-02},
  abstract = {The existence of recurrent synaptic connections in CA3 led to the hypothesis that CA3 is an autoassociative network similar to the Hopfield networks studied by theorists. CA3 undergoes gamma frequency periodic inhibition that prevents a persistent attractor state. This argues against the analogy to Hopfield nets, in which an attractor state can be used for working memory. However, we show that such periodic inhibition allows one cycle of recurrent excitatory activity and that this is sufficient for memory retrieval (within milliseconds). Thus, gamma oscillations are compatible with a long-term autoassociative memory function for CA3. A second goal of our work was to evaluate previous methods for estimating the memory capacity (P) of CA3. We confirm the equation, P = c/a2, where c is the probability that any two cells are recurrently connected and a is the fraction of cells representing a memory item. In applying this to CA3, we focus on CA3a, the subregion where recurrent connections are most numerous (c = 0.2) and approximate randomness. We estimate that a memory item is represented by ∼225 of the 70,000 neurons in CA3a (a = 0.003) and that ∼20,000 memory items can be stored. Our general conclusion is that the physiological and anatomical findings of CA3a are consistent with an autoassociative function. The nature of the information that is associated in CA3a is discussed. We also discuss how the autoassociative properties of CA3 and the heteroassociative properties of dentate synapses (linking sequential memories) form an integrated system for the storage and recall of item sequences. The recall process generates the phase precession in dentate, CA3, and entorhinal cortex.},
  pmcid = {PMC2080581},
  file = {/Users/marcel/Zotero/storage/6AES4753/de Almeida et al. - 2007 - Memory retrieval time and memory capacity of the C.pdf}
}

@unpublished{dehaanGaugeEquivariantMesh2020,
  title = {Gauge {{Equivariant Mesh CNNs}}: {{Anisotropic}} Convolutions on Geometric Graphs},
  shorttitle = {Gauge {{Equivariant Mesh CNNs}}},
  author = {de Haan, Pim and Weiler, Maurice and Cohen, Taco and Welling, Max},
  options = {useprefix=true},
  date = {2020-03-11},
  eprint = {2003.05425},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.05425},
  urldate = {2021-07-27},
  abstract = {A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/G2B3JR9H/de Haan et al_2020_Gauge Equivariant Mesh CNNs.pdf;/Users/marcel/Zotero/storage/XE4AK58X/2003.html}
}

@unpublished{dengVectorNeuronsGeneral2021,
  title = {Vector {{Neurons}}: {{A General Framework}} for {{SO}}(3)-{{Equivariant Networks}}},
  shorttitle = {Vector {{Neurons}}},
  author = {Deng, Congyue and Litany, Or and Duan, Yueqi and Poulenard, Adrien and Tagliasacchi, Andrea and Guibas, Leonidas},
  date = {2021-04-25},
  eprint = {2104.12229},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.12229},
  urldate = {2021-07-30},
  abstract = {Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/P3P967GI/Deng et al_2021_Vector Neurons.pdf;/Users/marcel/Zotero/storage/5BRHQZ3G/2104.html}
}

@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  date = {2021-06-01},
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05233},
  url = {http://arxiv.org/abs/2105.05233},
  urldate = {2022-07-11},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/YL7TB8WE/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf;/Users/marcel/Zotero/storage/ZD4BNQJX/2105.html}
}

@misc{dietrichStructuredDynamicSparse2021,
  title = {Towards {{Structured Dynamic Sparse Pre-Training}} of {{BERT}}},
  author = {Dietrich, Anastasia and Gressmann, Frithjof and Orr, Douglas and Chelombiev, Ivan and Justus, Daniel and Luschi, Carlo},
  date = {2021-08-13},
  number = {arXiv:2108.06277},
  eprint = {2108.06277},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.06277},
  url = {http://arxiv.org/abs/2108.06277},
  urldate = {2022-09-23},
  abstract = {Identifying algorithms for computational efficient unsupervised training of large language models is an important and active area of research. In this work, we develop and study a straightforward, dynamic always-sparse pre-training approach for BERT language modeling task, which leverages periodic compression steps based on magnitude pruning followed by random parameter re-allocation. This approach enables us to achieve Pareto improvements in terms of the number of floating-point operations (FLOPs) over statically sparse and dense models across a broad spectrum of network sizes. Furthermore, we demonstrate that training remains FLOP-efficient when using coarse-grained block sparsity, making it particularly promising for efficient execution on modern hardware accelerators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/6DLDSHU8/Dietrich et al. - 2021 - Towards Structured Dynamic Sparse Pre-Training of .pdf;/Users/marcel/Zotero/storage/XG7XCCZT/2108.html}
}

@misc{digiovanniGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} as {{Gradient Flows}}},
  author = {Di Giovanni, Francesco and Rowbottom, James and Chamberlain, Benjamin P. and Markovich, Thomas and Bronstein, Michael M.},
  date = {2022-08-15},
  number = {arXiv:2206.10991},
  eprint = {2206.10991},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.10991},
  url = {http://arxiv.org/abs/2206.10991},
  urldate = {2022-08-18},
  abstract = {Dynamical systems minimizing an energy are ubiquitous in geometry and physics. We propose a novel framework for GNNs where we parametrize (and \{\textbackslash em learn\}) an energy functional and then take the GNN equations to be the gradient flow of such energy. This approach allows to analyse the GNN evolution from a multi-particle perspective as learning attractive and repulsive forces in feature space via the positive and negative eigenvalues of a symmetric `channel-mixing' matrix. We conduct spectral analysis of the solutions and provide a better understanding of the role of the channel-mixing in (residual) graph convolutional models and of its ability to steer the diffusion away from over-smoothing. We perform thorough ablation studies corroborating our theory and show competitive performance of simple models on homophilic and heterophilic datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/DEYWR5W7/Di Giovanni et al. - 2022 - Graph Neural Networks as Gradient Flows.pdf;/Users/marcel/Zotero/storage/YVYBFTAZ/2206.html}
}

@article{digiovanniRicciFlowWarped2020,
  title = {Ricci Flow of Warped {{Berger}} Metrics on \$\textbackslash mathbb\{\vphantom\}{{R}}\vphantom\{\}\^\{4\}\$},
  author = {Di Giovanni, Francesco},
  date = {2020-10},
  journaltitle = {Calculus of Variations and Partial Differential Equations},
  shortjournal = {Calc. Var.},
  volume = {59},
  number = {5},
  eprint = {1904.02236},
  eprinttype = {arxiv},
  pages = {162},
  issn = {0944-2669, 1432-0835},
  doi = {10.1007/s00526-020-01823-4},
  url = {http://arxiv.org/abs/1904.02236},
  urldate = {2022-03-31},
  abstract = {We study the Ricci flow on \$\textbackslash mathbb\{R\}\^\{4\}\$ starting at an SU(2)-cohomogeneity 1 metric \$g\_\{0\}\$ whose restriction to any hypersphere is a Berger metric. We prove that if \$g\_\{0\}\$ has no necks and is bounded by a cylinder, then the solution develops a global Type-II singularity and converges to the Bryant soliton when suitably dilated at the origin. This is the first example in dimension \$n {$>$} 3\$ of a non-rotationally symmetric Type-II flow converging to a rotationally symmetric singularity model. Next, we show that if instead \$g\_\{0\}\$ has no necks, its curvature decays and the Hopf fibers are not collapsed, then the solution is immortal. Finally, we prove that if the flow is Type-I, then there exist minimal 3-spheres for times close to the maximal time.},
  archiveprefix = {arXiv},
  keywords = {bronstein,Mathematics - Differential Geometry},
  file = {/Users/marcel/Zotero/storage/89IBK4BU/Di Giovanni - 2020 - Ricci flow of warped Berger metrics on $mathbb R .pdf;/Users/marcel/Zotero/storage/TK8QY7UF/1904.html}
}

@article{dormandFamilyEmbeddedRungeKutta1980,
  title = {A Family of Embedded {{Runge-Kutta}} Formulae},
  author = {Dormand, J. R. and Prince, P. J.},
  date = {1980-03-01},
  journaltitle = {Journal of Computational and Applied Mathematics},
  shortjournal = {Journal of Computational and Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {19--26},
  issn = {0377-0427},
  doi = {10.1016/0771-050X(80)90013-3},
  url = {https://www.sciencedirect.com/science/article/pii/0771050X80900133},
  urldate = {2022-09-11},
  abstract = {A family of embedded Runge-Kutta formulae RK5 (4) are derived. From these are presented formulae which have (a) ‘small’ principal truncation terms in the fifth order and (b) extended regions of absolute stability.},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/X8IK5MTD/Dormand and Prince - 1980 - A family of embedded Runge-Kutta formulae.pdf;/Users/marcel/Zotero/storage/TPY7XPIA/0771050X80900133.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2022-09-22},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/SVQGXN9Z/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/marcel/Zotero/storage/D4EHHIRJ/2010.html}
}

@unpublished{dupontGenerativeModelsDistributions2021,
  title = {Generative {{Models}} as {{Distributions}} of {{Functions}}},
  author = {Dupont, Emilien and Teh, Yee Whye and Doucet, Arnaud},
  date = {2021-05-30},
  eprint = {2102.04776},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2102.04776},
  urldate = {2021-07-27},
  abstract = {Generative models are typically trained on grid-like data such as images. As a result, the size of these models usually scales directly with the underlying grid resolution. In this paper, we abandon discretized grids and instead parameterize individual data points by continuous functions. We then build generative models by learning distributions over such functions. By treating data points as functions, we can abstract away from the specific type of data we train on and construct models that scale independently of signal resolution. To train our model, we use an adversarial approach with a discriminator that acts on continuous signals. Through experiments on both images and 3D shapes, we demonstrate that our model can learn rich distributions of functions independently of data type and resolution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/ARW3UVTR/Dupont et al_2021_Generative Models as Distributions of Functions.pdf;/Users/marcel/Zotero/storage/8LFKZE3I/2102.html}
}

@unpublished{finziGeneralizingConvolutionalNeural2020,
  title = {Generalizing {{Convolutional Neural Networks}} for {{Equivariance}} to {{Lie Groups}} on {{Arbitrary Continuous Data}}},
  author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  date = {2020-09-24},
  eprint = {2002.12880},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.12880},
  urldate = {2021-07-27},
  abstract = {The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/X4XII54R/Finzi et al_2020_Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on.pdf;/Users/marcel/Zotero/storage/RK6IYEJR/2002.html}
}

@unpublished{finziPracticalMethodConstructing2021,
  title = {A {{Practical Method}} for {{Constructing Equivariant Multilayer Perceptrons}} for {{Arbitrary Matrix Groups}}},
  author = {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
  date = {2021-04-19},
  eprint = {2104.09459},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2104.09459},
  urldate = {2021-07-27},
  abstract = {Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including \$\textbackslash mathrm\{O\}(1,3)\$, \$\textbackslash mathrm\{O\}(5)\$, \$\textbackslash mathrm\{Sp\}(n)\$, and the Rubik's cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary matrix groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/Y6SQAPRT/Finzi et al_2021_A Practical Method for Constructing Equivariant Multilayer Perceptrons for.pdf;/Users/marcel/Zotero/storage/BJLY5LS4/2104.html}
}

@inproceedings{franzenGeneralNonlinearitiesEquivariant2021,
  title = {General {{Nonlinearities}} in {{SO}}(2)-{{Equivariant CNNs}}},
  author = {Franzen, Daniel and Wand, Michael},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=PFBHMlpaWY},
  urldate = {2022-05-08},
  abstract = {We improve the ability of using (more) general nonlinerities in SO(2)-equivariant steerable networks.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/SCXMJVJM/Franzen and Wand - 2021 - General Nonlinearities in SO(2)-Equivariant CNNs.pdf;/Users/marcel/Zotero/storage/SX6VGLXZ/forum.html}
}

@unpublished{fuchsSETransformers3D2020,
  title = {{{SE}}(3)-{{Transformers}}: {{3D Roto-Translation Equivariant Attention Networks}}},
  shorttitle = {{{SE}}(3)-{{Transformers}}},
  author = {Fuchs, Fabian B. and Worrall, Daniel E. and Fischer, Volker and Welling, Max},
  date = {2020-11-24},
  eprint = {2006.10503},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.10503},
  urldate = {2021-07-27},
  abstract = {We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/C6WXLBFZ/Fuchs et al_2020_SE(3)-Transformers.pdf;/Users/marcel/Zotero/storage/TH5IPHLX/2006.html}
}

@article{fukushimaNeocognitronSelforganizingNeural1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  date = {1980-04-01},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  url = {https://doi.org/10.1007/BF00344251},
  urldate = {2022-04-29},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  langid = {english},
  keywords = {Complex Cell,Digital Computer,Input Layer,Neural Network Model,Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/TCYLX4RU/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf}
}

@misc{geigerE3nnEuclideanNeural2022,
  title = {E3nn: {{Euclidean Neural Networks}}},
  shorttitle = {E3nn},
  author = {Geiger, Mario and Smidt, Tess},
  date = {2022-07-18},
  number = {arXiv:2207.09453},
  eprint = {2207.09453},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.09453},
  url = {http://arxiv.org/abs/2207.09453},
  urldate = {2022-08-18},
  abstract = {We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marcel/Zotero/storage/HT3MRT7X/Geiger and Smidt - 2022 - e3nn Euclidean Neural Networks.pdf;/Users/marcel/Zotero/storage/5KZT2K3J/2207.html}
}

@misc{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-06-12},
  number = {arXiv:1704.01212},
  eprint = {1704.01212},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.01212},
  url = {http://arxiv.org/abs/1704.01212},
  urldate = {2022-09-22},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.6},
  file = {/Users/marcel/Zotero/storage/ZLNXH4BF/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf;/Users/marcel/Zotero/storage/KWRUWNR8/1704.html}
}

@unpublished{golkarBiologicallyPlausibleNeural2020,
  title = {A Biologically Plausible Neural Network for Local Supervision in Cortical Microcircuits},
  author = {Golkar, Siavash and Lipshutz, David and Bahroun, Yanis and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
  date = {2020-11-30},
  eprint = {2011.15031},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2011.15031},
  urldate = {2022-04-19},
  abstract = {The backpropagation algorithm is an invaluable tool for training artificial neural networks; however, because of a weight sharing requirement, it does not provide a plausible model of brain function. Here, in the context of a two-layer network, we derive an algorithm for training a neural network which avoids this problem by not requiring explicit error computation and backpropagation. Furthermore, our algorithm maps onto a neural network that bears a remarkable resemblance to the connectivity structure and learning rules of the cortex. We find that our algorithm empirically performs comparably to backprop on a number of datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/marcel/Zotero/storage/NXPC3TM6/Golkar et al. - 2020 - A biologically plausible neural network for local .pdf;/Users/marcel/Zotero/storage/GAWKW9S3/2011.html}
}

@unpublished{grathwohlFFJORDFreeformContinuous2018,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  date = {2018-10-22},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.01367},
  urldate = {2021-07-27},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/3L6TCNCW/Grathwohl et al_2018_FFJORD.pdf;/Users/marcel/Zotero/storage/CBG8FBER/1810.html}
}

@book{hebbOrganizationBehaviorNeuropsychological2002,
  title = {The Organization of Behavior: A Neuropsychological Theory},
  shorttitle = {The Organization of Behavior},
  author = {Hebb, D. O.},
  date = {2002},
  publisher = {{L. Erlbaum Associates}},
  location = {{Mahwah, N.J}},
  isbn = {978-0-8058-4300-2},
  pagetotal = {335},
  keywords = {Neuropsychology}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  doi = {10.48550/arXiv.1512.03385},
  url = {https://arxiv.org/abs/1512.03385v1},
  urldate = {2022-05-02},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/FFXV9QXD/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/marcel/Zotero/storage/PS9P3KJ6/1512.html}
}

@inproceedings{heLearningFilterPruning2020,
  title = {Learning {{Filter Pruning Criteria}} for {{Deep Convolutional Neural Networks Acceleration}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  date = {2020-06},
  pages = {2006--2015},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00208},
  abstract = {Filter pruning has been widely applied to neural network compression and acceleration. Existing methods usually utilize pre-defined pruning criteria, such as Lp-norm, to prune unimportant filters. There are two major limitations to these methods. First, existing methods fail to consider the variety of filter distribution across layers. To extract features of the coarse level to the fine level, the filters of different layers have various distributions. Therefore, it is not suitable to utilize the same pruning criteria to different functional layers. Second, prevailing layer-by-layer pruning methods process each layer independently and sequentially, failing to consider that all the layers in the network collaboratively make the final prediction. In this paper, we propose Learning Filter Pruning Criteria (LFPC) to solve the above problems. Specifically, we develop a differentiable pruning criteria sampler. This sampler is learnable and optimized by the validation loss of the pruned network obtained from the sampled criteria. In this way, we could adaptively select the appropriate pruning criteria for different functional layers. Besides, when evaluating the sampled criteria, LFPC comprehensively consider the contribution of all the layers at the same time. Experiments validate our approach on three image classification benchmarks. Notably, on ILSVRC-2012, our LFPC reduces more than 60\% FLOPs on ResNet-50 with only 0.83\% top-5 accuracy loss.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Acceleration,Benchmark testing,Computer architecture,Computer vision,Convolutional neural networks,Feature extraction,Training},
  file = {/Users/marcel/Zotero/storage/W64S37SN/He et al_2020_Learning Filter Pruning Criteria for Deep Convolutional Neural Networks.pdf;/Users/marcel/Zotero/storage/NMGUANR3/9156434.html}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2022-07-11},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/UL35LDUM/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/marcel/Zotero/storage/N3WK8QJ4/2006.html}
}

@article{hodgkinQuantitativeDescriptionMembrane1952a,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  date = {1952-08-28},
  journaltitle = {The Journal of Physiology},
  shortjournal = {J Physiol},
  volume = {117},
  number = {4},
  eprint = {12991237},
  eprinttype = {pmid},
  pages = {500--544},
  issn = {0022-3751},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/},
  urldate = {2022-05-01},
  pmcid = {PMC1392413},
  file = {/Users/marcel/Zotero/storage/N82XQIU2/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J J},
  date = {1982-04},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  volume = {79},
  number = {8},
  eprint = {6953413},
  eprinttype = {pmid},
  pages = {2554--2558},
  issn = {0027-8424},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
  urldate = {2022-05-02},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  pmcid = {PMC346238}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2022-04-28},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/marcel/Zotero/storage/LFVE5I66/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf;/Users/marcel/Zotero/storage/UW7CBZGB/0893608089900208.html}
}

@book{hubelBrainVisualPerception2005,
  title = {Brain and Visual Perception: The Story of a 25-Year Collaboration},
  shorttitle = {Brain and Visual Perception},
  author = {Hubel, David H. and Wiesel, Torsten N.},
  date = {2005},
  publisher = {{Oxford University Press}},
  location = {{New York, N.Y}},
  isbn = {978-0-19-517618-6},
  pagetotal = {729},
  keywords = {Biomedical Research,history,History of Medicine; 20th Cent,physiology,United States,Visual pathways,Visual perception,Visual Perception}
}

@article{hubelReceptiveFieldsSingle1959,
  title = {Receptive Fields of Single Neurons in the Cat's Striate Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  date = {1959-10-01},
  journaltitle = {The Journal of Physiology},
  volume = {148},
  number = {3},
  pages = {574--591},
  issn = {00223751},
  doi = {10.1113/jphysiol.1959.sp006308},
  url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1959.sp006308},
  urldate = {2022-04-29},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/D458RJIH/Hubel and Wiesel - 1959 - Receptive fields of single neurons in the cat's st.pdf}
}

@unpublished{hutchinsonLieTransformerEquivariantSelfattention2021,
  title = {{{LieTransformer}}: {{Equivariant}} Self-Attention for {{Lie Groups}}},
  shorttitle = {{{LieTransformer}}},
  author = {Hutchinson, Michael and Lan, Charline Le and Zaidi, Sheheryar and Dupont, Emilien and Teh, Yee Whye and Kim, Hyunjik},
  date = {2021-06-16},
  eprint = {2012.10885},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2012.10885},
  urldate = {2021-07-27},
  abstract = {Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to self-attention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/BDIDM8AK/Hutchinson et al_2021_LieTransformer.pdf;/Users/marcel/Zotero/storage/MVSMJ3WL/2012.html}
}

@article{illingBiologicallyPlausibleDeep2019,
  title = {Biologically Plausible Deep Learning — {{But}} How Far Can We Go with Shallow Networks?},
  author = {Illing, Bernd and Gerstner, Wulfram and Brea, Johanni},
  date = {2019-10-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {118},
  pages = {90--101},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.06.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608019301741},
  urldate = {2022-04-19},
  abstract = {Training deep neural networks with the error backpropagation algorithm is considered implausible from a biological perspective. Numerous recent publications suggest elaborate models for biologically plausible variants of deep learning, typically defining success as reaching around 98\% test accuracy on the MNIST data set. Here, we investigate how far we can go on digit (MNIST) and object (CIFAR10) classification with biologically plausible, local learning rules in a network with one hidden layer and a single readout layer. The hidden layer weights are either fixed (random or random Gabor filters) or trained with unsupervised methods (Principal/Independent Component Analysis or Sparse Coding) that can be implemented by local learning rules. The readout layer is trained with a supervised, local learning rule. We first implement these models with rate neurons. This comparison reveals, first, that unsupervised learning does not lead to better performance than fixed random projections or Gabor filters for large hidden layers. Second, networks with localized receptive fields perform significantly better than networks with all-to-all connectivity and can reach backpropagation performance on MNIST. We then implement two of the networks – fixed, localized, random \& random Gabor filters in the hidden layer – with spiking leaky integrate-and-fire neurons and spike timing dependent plasticity to train the readout layer. These spiking models achieve {$>$}98.2\% test accuracy on MNIST, which is close to the performance of rate networks with one hidden layer trained with backpropagation. The performance of our shallow network models is comparable to most current biologically plausible models of deep learning. Furthermore, our results with a shallow spiking network provide an important reference and suggest the use of data sets other than MNIST for testing the performance of future models of biologically plausible deep learning.},
  langid = {english},
  keywords = {Deep learning,Local learning rules,MNIST,Random projections,Spiking networks,Unsupervised feature learning},
  file = {/Users/marcel/Zotero/storage/NN98A8JI/Illing et al. - 2019 - Biologically plausible deep learning — But how far.pdf}
}

@misc{IMM2012-03274,
  title = {The Matrix Cookbook},
  author = {Petersen, K. B. and Pedersen, M. S.},
  date = {2012-11},
  publisher = {{Technical University of Denmark}},
  url = {http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html},
  abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
  keywords = {inverse,matrix derivative,Matrix identity,matrix relations}
}

@software{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  date = {2018},
  url = {http://github.com/google/jax},
  version = {0.2.5}
}

@online{JaxLaxScatter,
  title = {Jax.Lax.Scatter — {{JAX}} Documentation},
  url = {https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html},
  urldate = {2022-09-23},
  file = {/Users/marcel/Zotero/storage/IQ522L6U/jax.lax.scatter.html}
}

@unpublished{jennerSteerablePartialDifferential2021,
  title = {Steerable {{Partial Differential Operators}} for {{Equivariant Neural Networks}}},
  author = {Jenner, Erik and Weiler, Maurice},
  date = {2021-06-18},
  eprint = {2106.10163},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.10163},
  urldate = {2021-07-30},
  abstract = {Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a \$G\$-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups \$G\$. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/T8IMR2EM/Jenner_Weiler_2021_Steerable Partial Differential Operators for Equivariant Neural Networks.pdf;/Users/marcel/Zotero/storage/RUCQG2MB/2106.html}
}

@article{kidger2021equinox,
  title = {Equinox: Neural Networks in {{JAX}} via Callable {{PyTrees}} and Filtered Transformations},
  author = {Kidger, Patrick and Garcia, Cristian},
  date = {2021},
  journaltitle = {Differentiable Programming workshop at Neural Information Processing Systems 2021}
}

@unpublished{kidgerNeuralDifferentialEquations2022,
  title = {On {{Neural Differential Equations}}},
  author = {Kidger, Patrick},
  date = {2022-02-04},
  eprint = {2202.02435},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2202.02435},
  urldate = {2022-03-31},
  abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/5VANUP73/Kidger - 2022 - On Neural Differential Equations.pdf;/Users/marcel/Zotero/storage/TIV7GPSA/2202.html}
}

@article{kimVisualExplanationsSpiking2021,
  title = {Visual Explanations from Spiking Neural Networks Using Inter-Spike Intervals},
  author = {Kim, Youngeun and Panda, Priyadarshini},
  date = {2021-09-24},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {19037},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-98448-0},
  url = {https://www.nature.com/articles/s41598-021-98448-0},
  urldate = {2022-05-02},
  abstract = {By emulating biological features in brain, Spiking Neural Networks (SNNs) offer an energy-efficient alternative to conventional deep learning. To make SNNs ubiquitous, a ‘visual explanation’ technique for analysing and explaining the internal spike behavior of such temporal deep SNNs is crucial. Explaining SNNs visually will make the network more transparent giving the end-user a tool to understand how SNNs make temporal predictions and why they make a certain decision. In this paper, we propose a bio-plausible visual explanation tool for SNNs, called Spike Activation Map (SAM). SAM yields a heatmap (i.e., localization map) corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without the use of gradients and ground truth, SAM produces a temporal localization map highlighting the region of interest in an image attributed to an SNN’s prediction at each time-step. Overall, SAM outsets the beginning of a new research area ‘explainable neuromorphic computing’ that will ultimately allow end-users to establish appropriate trust in predictions from SNNs.},
  issue = {1},
  langid = {english},
  keywords = {Computational neuroscience,Electrical and electronic engineering,Visual system},
  file = {/Users/marcel/Zotero/storage/I6UXW6C8/Kim and Panda - 2021 - Visual explanations from spiking neural networks u.pdf;/Users/marcel/Zotero/storage/IBDW4J2C/s41598-021-98448-0.html}
}

@unpublished{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-04-28},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/ER5IRCM4/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/marcel/Zotero/storage/34E5ZICM/1412.html}
}

@unpublished{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2021-07-27},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/SE6H8TVJ/Kingma_Dhariwal_2018_Glow.pdf;/Users/marcel/Zotero/storage/DMGVNZCE/1807.html}
}

@article{kobyzevNormalizingFlowsIntroduction2020,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  date = {2020},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  url = {http://arxiv.org/abs/1908.09257},
  urldate = {2021-07-27},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/G8BAB6TM/Kobyzev et al_2020_Normalizing Flows.pdf;/Users/marcel/Zotero/storage/93628RDS/1908.html}
}

@unpublished{kohlerEquivariantFlowsExact2020,
  title = {Equivariant {{Flows}}: {{Exact Likelihood Generative Learning}} for {{Symmetric Densities}}},
  shorttitle = {Equivariant {{Flows}}},
  author = {Köhler, Jonas and Klein, Leon and Noé, Frank},
  date = {2020-10-26},
  eprint = {2006.02425},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  url = {http://arxiv.org/abs/2006.02425},
  urldate = {2021-07-27},
  abstract = {Normalizing flows are exact-likelihood generative neural networks which approximately transform samples from a simple prior distribution to samples of the probability distribution of interest. Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. To scale and generalize these results, it is essential that the natural symmetries in the probability density -- in physics defined by the invariances of the target potential -- are built into the flow. We provide a theoretical sufficient criterion showing that the distribution generated by \textbackslash textit\{equivariant\} normalizing flows is invariant with respect to these symmetries by design. Furthermore, we propose building blocks for flows which preserve symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/VP3GL86X/Köhler et al_2020_Equivariant Flows.pdf;/Users/marcel/Zotero/storage/CHRE346C/2006.html}
}

@unpublished{kondorClebschGordanNetsFully2018,
  title = {Clebsch-{{Gordan Nets}}: A {{Fully Fourier Space Spherical Convolutional Neural Network}}},
  shorttitle = {Clebsch-{{Gordan Nets}}},
  author = {Kondor, Risi and Lin, Zhen and Trivedi, Shubhendu},
  date = {2018-11-10},
  eprint = {1806.09231},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.09231},
  urldate = {2021-08-27},
  abstract = {Recent work by Cohen \textbackslash emph\{et al.\} has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/VBDXVEN8/Kondor et al_2018_Clebsch-Gordan Nets.pdf;/Users/marcel/Zotero/storage/WQEZITRD/1806.html}
}

@article{krotovUnsupervisedLearningCompeting2019,
  title = {Unsupervised Learning by Competing Hidden Units},
  author = {Krotov, Dmitry and Hopfield, John J.},
  date = {2019-04-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {116},
  number = {16},
  pages = {7723--7731},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820458116},
  url = {https://pnas.org/doi/full/10.1073/pnas.1820458116},
  urldate = {2022-05-01},
  abstract = {Significance             Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility.           ,              It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb’s idea that change of the synapse strength should be local—i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks.},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/GEIWKJSW/Krotov and Hopfield - 2019 - Unsupervised learning by competing hidden units.pdf}
}

@misc{laiRobustVectorQuantizedVariational2022,
  title = {Robust {{Vector Quantized-Variational Autoencoder}}},
  author = {Lai, Chieh-Hsin and Zou, Dongmian and Lerman, Gilad},
  date = {2022-02-04},
  number = {arXiv:2202.01987},
  eprint = {2202.01987},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.01987},
  url = {http://arxiv.org/abs/2202.01987},
  urldate = {2022-07-31},
  abstract = {Image generative models can learn the distributions of the training data and consequently generate examples by sampling from these distributions. However, when the training dataset is corrupted with outliers, generative models will likely produce examples that are also similar to the outliers. In fact, a small portion of outliers may induce state-of-the-art generative models, such as Vector Quantized-Variational AutoEncoder (VQ-VAE), to learn a significant mode from the outliers. To mitigate this problem, we propose a robust generative model based on VQ-VAE, which we name Robust VQ-VAE (RVQ-VAE). In order to achieve robustness, RVQ-VAE uses two separate codebooks for the inliers and outliers. To ensure the codebooks embed the correct components, we iteratively update the sets of inliers and outliers during each training epoch. To ensure that the encoded data points are matched to the correct codebooks, we quantize using a weighted Euclidean distance, whose weights are determined by directional variances of the codebooks. Both codebooks, together with the encoder and decoder, are trained jointly according to the reconstruction loss and the quantization loss. We experimentally demonstrate that RVQ-VAE is able to generate examples from inliers even if a large portion of the training data points are corrupted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/AZLANL5P/Lai et al. - 2022 - Robust Vector Quantized-Variational Autoencoder.pdf;/Users/marcel/Zotero/storage/QUCMTDTQ/2202.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/marcel/Zotero/storage/CE6JRHB8/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/marcel/Zotero/storage/IWA9SBAC/726791.html}
}

@article{liAdaptiveDropoutMethod2021a,
  title = {Adaptive {{Dropout Method Based}} on {{Biological Principles}}},
  author = {Li, Hailiang and Weng, Jian and Mao, Yijun and Wang, Yonghua and Zhan, Yiju and Cai, Qingling and Gu, Wanrong},
  date = {2021-09},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {9},
  pages = {4267--4276},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3070895},
  abstract = {Dropout is one of the most widely used methods to avoid overfitting neural networks. However, it rigidly and randomly activates neurons according to a fixed probability, which is not consistent with the activation mode of neurons in the human cerebral cortex. Inspired by gene theory and the activation mechanism of brain neurons, we propose a more intelligent adaptive dropout, in which a variational self-encoder (VAE) overlaps to an existing neural network to regularize its hidden neurons by adaptively setting activities to zero. Through alternating iterative training, the discarding probability of each hidden neuron can be learned according to the weights and thus effectively avoid the shortcomings of the standard dropout method. The experimental results in multiple data sets illustrate that this method can better suppress overfitting in various neural networks than can the standard dropout. Additionally, this adaptive dropout technique can reduce the number of neurons and improve training efficiency.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Adaptive dropout,Adaptive systems,Biological neural networks,deep learning,dropout,Feature extraction,Neurons,overfitting,Standards,Training,Unsupervised learning},
  file = {/Users/marcel/Zotero/storage/2R3ZPAG5/Li et al. - 2021 - Adaptive Dropout Method Based on Biological Princi.pdf;/Users/marcel/Zotero/storage/N2X7RWCD/9408375.html}
}

@article{lillicrapBackpropagationBrain2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  date = {2020-06},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {21},
  number = {6},
  pages = {335--346},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  url = {https://www.nature.com/articles/s41583-020-0277-3},
  urldate = {2022-04-30},
  abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
  issue = {6},
  langid = {english},
  keywords = {Cortex,Learning algorithms,Long-term potentiation,Network models,Neurophysiology},
  file = {/Users/marcel/Zotero/storage/HTRK6RNQ/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf;/Users/marcel/Zotero/storage/Z3QA3YBE/s41583-020-0277-3.html}
}

@misc{linAcceleratingSpMMKernel2021,
  title = {Accelerating {{SpMM Kernel}} with {{Cache-First Edge Sampling}} for {{Graph Neural Networks}}},
  author = {Lin, Chien-Yu and Luo, Liang and Ceze, Luis},
  date = {2021-04-23},
  number = {arXiv:2104.10716},
  eprint = {2104.10716},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2104.10716},
  urldate = {2022-09-23},
  abstract = {Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of applications. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95\% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs' applicability to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU's shared memory. It thus reduces the computation cost and improves SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1\% accuracy loss.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/KSTVAV36/Lin et al. - 2021 - Accelerating SpMM Kernel with Cache-First Edge Sam.pdf;/Users/marcel/Zotero/storage/6WVHY6VA/2104.html}
}

@unpublished{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  date = {2017-03-10},
  eprint = {1608.08710},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1608.08710},
  urldate = {2021-07-27},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/M33784WY/Li et al_2017_Pruning Filters for Efficient ConvNets.pdf;/Users/marcel/Zotero/storage/7D76A2H2/1608.html}
}

@article{lipshutzBiologicallyPlausibleNeural2021,
  title = {A {{Biologically Plausible Neural Network}} for {{Multichannel Canonical Correlation Analysis}}},
  author = {Lipshutz, David and Bahroun, Yanis and Golkar, Siavash and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
  date = {2021-08-19},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  volume = {33},
  number = {9},
  eprint = {34412114},
  eprinttype = {pmid},
  pages = {2309--2352},
  issn = {1530-888X},
  doi = {10.1162/neco_a_01414},
  abstract = {Cortical pyramidal neurons receive inputs from multiple distinct neural populations and integrate these inputs in separate dendritic compartments. We explore the possibility that cortical microcircuits implement canonical correlation analysis (CCA), an unsupervised learning method that projects the inputs onto a common subspace so as to maximize the correlations between the projections. To this end, we seek a multichannel CCA algorithm that can be implemented in a biologically plausible neural network. For biological plausibility, we require that the network operates in the online setting and its synaptic update rules are local. Starting from a novel CCA objective function, we derive an online optimization algorithm whose optimization steps can be implemented in a single-layer neural network with multicompartmental neurons and local non-Hebbian learning rules. We also derive an extension of our online CCA algorithm with adaptive output rank and output whitening. Interestingly, the extension maps onto a neural network whose neural architecture and synaptic updates resemble neural circuitry and non-Hebbian plasticity observed in the cortex.},
  langid = {english},
  keywords = {Algorithms,Canonical Correlation Analysis,Neural Networks; Computer,Neurons},
  file = {/Users/marcel/Zotero/storage/RQNMYR95/Lipshutz et al. - 2021 - A Biologically Plausible Neural Network for Multic.pdf}
}

@misc{liuGenerating3DMolecules2022,
  title = {Generating {{3D Molecules}} for {{Target Protein Binding}}},
  author = {Liu, Meng and Luo, Youzhi and Uchino, Kanji and Maruhashi, Koji and Ji, Shuiwang},
  date = {2022-05-30},
  number = {arXiv:2204.09410},
  eprint = {2204.09410},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.09410},
  url = {http://arxiv.org/abs/2204.09410},
  urldate = {2022-07-18},
  abstract = {A fundamental problem in drug discovery is to design molecules that bind to specific proteins. To tackle this problem using machine learning methods, here we propose a novel and effective framework, known as GraphBP, to generate 3D molecules that bind to given proteins by placing atoms of specific types and locations to the given binding site one by one. In particular, at each step, we first employ a 3D graph neural network to obtain geometry-aware and chemically informative representations from the intermediate contextual information. Such context includes the given binding site and atoms placed in the previous steps. Second, to preserve the desirable equivariance property, we select a local reference atom according to the designed auxiliary classifiers and then construct a local spherical coordinate system. Finally, to place a new atom, we generate its atom type and relative location w.r.t. the constructed local coordinate system via a flow model. We also consider generating the variables of interest sequentially to capture the underlying dependencies among them. Experiments demonstrate that our GraphBP is effective to generate 3D molecules with binding ability to target protein binding sites. Our implementation is available at https://github.com/divelab/GraphBP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/marcel/Zotero/storage/TJ6BTQ79/Liu et al. - 2022 - Generating 3D Molecules for Target Protein Binding.pdf;/Users/marcel/Zotero/storage/MBTIS33A/2204.html}
}

@unpublished{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date = {2016},
  volume = {9905},
  eprint = {1512.02325},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  url = {http://arxiv.org/abs/1512.02325},
  urldate = {2021-09-28},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/3EC4DG8A/Liu et al_2016_SSD.pdf;/Users/marcel/Zotero/storage/QGFH5FT6/1512.html}
}

@unpublished{lonesHowAvoidMachine2021,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2021-08-05},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2021-08-20},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/NQ47HWXN/Lones_2021_How to avoid machine learning pitfalls.pdf;/Users/marcel/Zotero/storage/AHZJAQNR/2108.html}
}

@unpublished{lotterDeepPredictiveCoding2017,
  title = {Deep {{Predictive Coding Networks}} for {{Video Prediction}} and {{Unsupervised Learning}}},
  author = {Lotter, William and Kreiman, Gabriel and Cox, David},
  date = {2017-02-28},
  eprint = {1605.08104},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1605.08104},
  urldate = {2022-05-01},
  abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/marcel/Zotero/storage/8KVI8XIK/Lotter et al. - 2017 - Deep Predictive Coding Networks for Video Predicti.pdf;/Users/marcel/Zotero/storage/U7UKLNNH/1605.html}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  date = {1997-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00011-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
  urldate = {2022-05-01},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  langid = {english},
  keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
  file = {/Users/marcel/Zotero/storage/P48KIM57/Maass - 1997 - Networks of spiking neurons The third generation .pdf;/Users/marcel/Zotero/storage/DH5BALMQ/S0893608097000117.html}
}

@video{machinelearningstreettalk041DrSIMON2021,
  title = {\#041 {{Dr}}. {{SIMON STRINGER}} - {{Biologically Plausible Neural Networks}}},
  editor = {{Machine Learning Street Talk}},
  date = {2021-02-03},
  url = {https://www.youtube.com/watch?v=aisgNLypUKs},
  urldate = {2022-04-19},
  abstract = {Dr. Simon Stringer. Obtained his Ph.D in mathematical state space control theory and has been a Senior Research Fellow at Oxford University for over 27 years. Simon is the director of the the Oxford Centre for Theoretical Neuroscience and Artificial Intelligence, which is based within the Oxford University Department of Experimental Psychology. His department covers vision, spatial processing, motor function, language and consciousness -- in particular -- how the primate visual system learns to make sense of complex natural scenes. Dr. Stringers laboratory houses a team of theoreticians, who are developing computer models of a range of different aspects of brain function. Simon's lab is investigating the neural and synaptic dynamics that underpin brain function. An important matter here is the The feature-binding problem which concerns how the visual system represents the hierarchical relationships between features. the visual system must represent hierarchical binding relations across the entire visual field at every spatial scale and level in the hierarchy of visual primitives. We discuss the emergence of self-organised behaviour, complex information processing, invariant sensory representations and hierarchical feature binding which emerges when you build biologically plausible neural networks with temporal spiking dynamics.  00:00:00 Tim Intro  00:09:31 Show kickoff  00:14:37 Hierarchical Feature binding and timing of action potentials  00:30:16 Hebb to Spike-timing-dependent plasticity (STDP)  00:35:27 Encoding of shape primitives  00:38:50 Is imagination working in the same place in the brain  00:41:12 Compare to supervised CNNs  00:45:59 Speech recognition, motor system, learning mazes  00:49:28 How practical are these spiking NNs  00:50:19 Why simulate the human brain  00:52:46 How much computational power do you gain from differential timings  00:55:08 Adversarial inputs  00:59:41 Generative / causal component needed?  01:01:46 Modalities of processing i.e. language  01:03:42 Understanding  01:04:37 Human hardware  01:06:19 Roadmap of NNs?  01:10:36 Intepretability methods for these new models  01:13:03 Won't GPT just scale and do this anyway?  01:15:51 What about trace learning and transformation learning  01:18:50 Categories of invariance  01:19:47 Biological plausibility  Pod version: https://anchor.fm/machinelearningstre... https://www.neuroscience.ox.ac.uk/res... https://en.wikipedia.org/wiki/Simon\_S... https://www.linkedin.com/in/simon-str... "A new approach to solving the feature-binding problem in primate vision" https://royalsocietypublishing.org/do... James B. Isbister, Akihiro Eguchi, Nasir Ahmad, Juan M. Galeazzi, Mark J. Buckley and Simon Stringer Simon's department is looking for funding, please do get in touch with him if you can facilitate this.  \#machinelearning \#neuroscience},
  editortype = {director}
}

@unpublished{mathieuRiemannianContinuousNormalizing2020,
  title = {Riemannian {{Continuous Normalizing Flows}}},
  author = {Mathieu, Emile and Nickel, Maximilian},
  date = {2020-12-09},
  eprint = {2006.10605},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.10605},
  urldate = {2021-07-27},
  abstract = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/7BL75Z5Q/Mathieu and Nickel - 2020 - Riemannian Continuous Normalizing Flows.pdf;/Users/marcel/Zotero/storage/WLWMBIXY/2006.html}
}

@article{mccarthyProposalDartmouthSummer1955,
  title = {A {{Proposal}} for the {{Dartmouth Summer Research Project}} on {{Artificial Intelligence}}, {{August}} 31, 1955},
  author = {McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
  date = {1955-08-31},
  journaltitle = {AI Magazine},
  volume = {27},
  number = {4},
  pages = {12--12},
  issn = {2371-9621},
  doi = {10.1609/aimag.v27i4.1904},
  url = {https://ojs.aaai.org/index.php/aimagazine/article/view/1904},
  urldate = {2022-05-01},
  abstract = {The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.},
  issue = {4},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/W7EZ6N7Q/McCarthy et al. - 2006 - A Proposal for the Dartmouth Summer Research Proje.pdf}
}

@article{mcclellandInteractiveActivationModel1981,
  title = {An Interactive Activation Model of Context Effects in Letter Perception: {{I}}. {{An}} Account of Basic Findings},
  shorttitle = {An Interactive Activation Model of Context Effects in Letter Perception},
  author = {McClelland, James L. and Rumelhart, David E.},
  date = {1981},
  journaltitle = {Psychological Review},
  volume = {88},
  number = {5},
  pages = {375--407},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.88.5.375},
  abstract = {Describes a model in which perception results from excitatory and inhibitory interactions of detectors for visual features, letters, and words. A visual input excites detectors for visual features in the display and for letters consistent with the active features. Letter detectors in turn excite detectors for consistent words. It is suggested that active word detectors mutually inhibit each other and send feedback to the letter level, strengthening activation and hence perceptibility of their constituent letters. Computer simulation of the model exhibits the perceptual advantage for letters in words over unrelated contexts and is considered consistent with basic facts about word advantage. Most important, the model produces facilitation for letters in pronounceable pseudowords as well as words. Pseudowords activate detectors for words that are consistent with most active letters, and feedback from the activated words strengthens activations of the letters in the pseudoword. The model thus accounts for apparently rule-governed performance without any actual rules. (50 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Contextual Associations,Letters (Alphabet),Models,Visual Discrimination,Words (Phonetic Units)},
  file = {/Users/marcel/Zotero/storage/TMP6GHKW/1981-31825-001.html}
}

@article{mcculloch1943logical,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S and Pitts, Walter},
  date = {1943},
  journaltitle = {The bulletin of mathematical biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  publisher = {{Springer}},
  url = {https://home.csulb.edu/~cwallis/382/readings/482/mccolloch.logical.calculus.ideas.1943.pdf},
  file = {/Users/marcel/Zotero/storage/MFG4X8VH/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf}
}

@misc{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2022-07-31},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/marcel/Zotero/storage/GWM5FPJD/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf;/Users/marcel/Zotero/storage/2689R62Y/2003.html}
}

@unpublished{millidgePredictiveCodingApproximates2020,
  title = {Predictive {{Coding Approximates Backprop}} along {{Arbitrary Computation Graphs}}},
  author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  date = {2020-10-05},
  eprint = {2006.04182},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.04182},
  urldate = {2022-05-01},
  abstract = {Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marcel/Zotero/storage/XK7MBX4K/Millidge et al. - 2020 - Predictive Coding Approximates Backprop along Arbi.pdf;/Users/marcel/Zotero/storage/LQ4RJRQH/2006.html}
}

@book{minskyPerceptronsIntroductionComputational1969,
  title = {Perceptrons: {{An Introduction}} to {{Computational Geometry}}},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  date = {1969-01-15},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  isbn = {978-0-262-13043-1},
  langid = {english},
  pagetotal = {258},
  file = {/Users/marcel/Zotero/storage/NR5SNYHY/Minsky and Papert - 1969 - Perceptrons An Introduction to Computational Geom.pdf}
}

@report{mitchellNeedBiasesLearning1980,
  title = {The {{Need}} for {{Biases}} in {{Learning Generalizations}}},
  author = {Mitchell, Tom M.},
  date = {1980},
  abstract = {This paper defines precisely the notion of bias in generalization problems, then shows that biases are necessary for the inductive leap. Classes of justifiable biases are considered, and the relationship between bias and domain-independence is considered},
  file = {/Users/marcel/Zotero/storage/WUTLMF9M/Mitchell - 1980 - The Need for Biases in Learning Generalizations.pdf;/Users/marcel/Zotero/storage/LLGYVVCL/summary.html}
}

@inproceedings{monilUnderstandingImpactMemory2020,
  title = {Understanding the {{Impact}} of {{Memory Access Patterns}} in {{Intel Processors}}},
  booktitle = {2020 {{IEEE}}/{{ACM Workshop}} on {{Memory Centric High Performance Computing}} ({{MCHPC}})},
  author = {Monil, Mohammad Alaul Haque and Lee, Seyong and Vetter, Jeffrey S. and Malony, Allen D.},
  date = {2020-11},
  pages = {52--61},
  doi = {10.1109/MCHPC51950.2020.00012},
  abstract = {Because of increasing complexity in the memory hierarchy, predicting the performance of a given application in a given processor is becoming more difficult. The problem is worsened by the fact that the hardware needed to deal with more complex memory traffic also affects energy consumption. Moreover, in a heterogeneous system with shared main memory, the memory traffic between the last level cache (LLC) and the memory creates contention between other processors and accelerator devices. For these reasons, it is important to investigate and understand the impact of different memory access patterns on the memory system. This study investigates the interplay between Intel processors' memory hierarchy and different memory access patterns in applications. The authors explore sequential streaming and strided memory access patterns with the objective of predicting LLC-dynamic random access memory (DRAM) traffic for a given application in given Intel architectures. Moreover, the impact of prefetching is also investigated in this study. Experiments with different Intel micro-architectures uncover mechanisms to predict LLC-DRAM traffic that can yield up to 99\% accuracy for sequential streaming access patterns and up to 95\% accuracy for strided access patterns.},
  eventtitle = {2020 {{IEEE}}/{{ACM Workshop}} on {{Memory Centric High Performance Computing}} ({{MCHPC}})},
  keywords = {Broadwell,Cascade Lake,Hardware,Intel,Lakes,memory access patterns,Memory management,memory traffic prediction,Predictive models,Prefetching,Program processors,Random access memory,Sky Lake},
  file = {/Users/marcel/Zotero/storage/9SXQXGK4/Monil et al. - 2020 - Understanding the Impact of Memory Access Patterns.pdf}
}

@article{NeuralHarmonicFlow,
  title = {Neural {{Harmonic Flow}} on {{Graphs}}},
  file = {/Users/marcel/Zotero/storage/LGDF7B2H/Neural Harmonic Flow on Graphs.pdf}
}

@online{neurotrayHowManyCalculations2021,
  title = {How {{Many Calculations Per Second Can The Human Brain Do}}? - {{NeuroTray}}},
  shorttitle = {How {{Many Calculations Per Second Can The Human Brain Do}}?},
  author = {{neurotray}},
  date = {2021-02-14T05:47:25+00:00},
  url = {https://neurotray.com/how-many-calculations-per-second-can-the-human-brain-do/},
  urldate = {2022-05-02},
  abstract = {In this post we are going to answer the question ‘’How many calculations per second can the human brain do?’’ We will explain to you how the human brain has},
  langid = {american},
  file = {/Users/marcel/Zotero/storage/AVC79ZAE/how-many-calculations-per-second-can-the-human-brain-do.html}
}

@misc{nicholGLIDEPhotorealisticImage2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2022-03-08},
  number = {arXiv:2112.10741},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10741},
  url = {http://arxiv.org/abs/2112.10741},
  urldate = {2022-07-31},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/3C869CBV/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and.pdf;/Users/marcel/Zotero/storage/3B34IMYE/2112.html}
}

@misc{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  date = {2021-02-18},
  number = {arXiv:2102.09672},
  eprint = {2102.09672},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.09672},
  url = {http://arxiv.org/abs/2102.09672},
  urldate = {2022-07-11},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/ZK8PL3KK/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/marcel/Zotero/storage/K6MEX45Z/2102.html}
}

@unpublished{onkenOTFlowFastAccurate2021,
  title = {{{OT-Flow}}: {{Fast}} and {{Accurate Continuous Normalizing Flows}} via {{Optimal Transport}}},
  shorttitle = {{{OT-Flow}}},
  author = {Onken, Derek and Fung, Samy Wu and Li, Xingjian and Ruthotto, Lars},
  date = {2021-03-23},
  eprint = {2006.00104},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.00104},
  urldate = {2021-07-27},
  abstract = {A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/EQ9QSR8B/Onken et al_2021_OT-Flow.pdf;/Users/marcel/Zotero/storage/MVEVUS9L/2006.html}
}

@misc{oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2018-05-30},
  number = {arXiv:1711.00937},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.00937},
  url = {http://arxiv.org/abs/1711.00937},
  urldate = {2022-07-31},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/RLRPAV8B/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf;/Users/marcel/Zotero/storage/ZSW276D9/1711.html}
}

@software{optax2020github,
  title = {Optax: Composable Gradient Transformation and Optimisation, in {{JAX}}!},
  author = {Hessel, Matteo and Budden, David and Viola, Fabio and Rosca, Mihaela and Sezener, Eren and Hennigan, Tom},
  date = {2020},
  url = {http://github.com/deepmind/optax},
  version = {0.0.1}
}

@article{pfeifferDeepLearningSpiking2018,
  title = {Deep {{Learning With Spiking Neurons}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Deep {{Learning With Spiking Neurons}}},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  date = {2018},
  journaltitle = {Frontiers in Neuroscience},
  volume = {12},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00774},
  urldate = {2022-05-01},
  abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
  file = {/Users/marcel/Zotero/storage/7AHNGS48/Pfeiffer and Pfeil - 2018 - Deep Learning With Spiking Neurons Opportunities .pdf}
}

@article{pressAdaptiveStepsizeRungeKutta1992a,
  title = {Adaptive {{Stepsize Runge-Kutta Integration}}},
  author = {Press, William H. and Teukolsky, Saul A.},
  date = {1992},
  journaltitle = {Computers in Physics},
  shortjournal = {Comput. Phys.},
  volume = {6},
  number = {2},
  pages = {188},
  issn = {08941866},
  doi = {10.1063/1.4823060},
  url = {http://scitation.aip.org/content/aip/journal/cip/6/2/10.1063/1.4823060},
  urldate = {2022-09-11},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/73GNDZB6/Press and Teukolsky - 1992 - Adaptive Stepsize Runge-Kutta Integration.pdf}
}

@misc{qiPointNetDeepLearning2017,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  date = {2017-04-10},
  number = {arXiv:1612.00593},
  eprint = {1612.00593},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1612.00593},
  urldate = {2022-09-22},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/IBQQWE79/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf;/Users/marcel/Zotero/storage/KRKUN3L8/1612.html}
}

@inproceedings{rainaLargescaleDeepUnsupervised2009,
  title = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  date = {2009-06-14},
  series = {{{ICML}} '09},
  pages = {873--880},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553486},
  url = {https://doi.org/10.1145/1553374.1553486},
  urldate = {2022-05-01},
  abstract = {The promise of unsupervised learning methods lies in their potential to use vast amounts of unlabeled data to learn complex, highly nonlinear models with millions of free parameters. We consider two well-known unsupervised learning models, deep belief networks (DBNs) and sparse coding, that have recently been applied to a flurry of machine learning applications (Hinton \& Salakhutdinov, 2006; Raina et al., 2007). Unfortunately, current learning algorithms for both models are too slow for large-scale applications, forcing researchers to focus on smaller-scale models, or to use fewer training examples. In this paper, we suggest massively parallel methods to help resolve these problems. We argue that modern graphics processors far surpass the computational capabilities of multicore CPUs, and have the potential to revolutionize the applicability of deep unsupervised learning methods. We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors. We show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding. Our implementation of DBN learning is up to 70 times faster than a dual-core CPU implementation for large models. For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day. For sparse coding, we develop a simple, inherently parallel algorithm, that leads to a 5 to 15-fold speedup over previous methods.},
  isbn = {978-1-60558-516-1},
  file = {/Users/marcel/Zotero/storage/IYDFVUJ9/Raina et al. - 2009 - Large-scale deep unsupervised learning using graph.pdf}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2022-07-11},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/2CAARQGV/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;/Users/marcel/Zotero/storage/GBMIGUXP/2204.html}
}

@unpublished{ramsauerHopfieldNetworksAll2021,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2021-04-28},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2022-05-02},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/Z8RYFZ2I/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf;/Users/marcel/Zotero/storage/KNHN64FL/2008.html}
}

@article{raoPredictiveCodingVisual1999,
  title = {Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects},
  shorttitle = {Predictive Coding in the Visual Cortex},
  author = {Rao, Rajesh P. N. and Ballard, Dana H.},
  date = {1999-01},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {2},
  number = {1},
  pages = {79--87},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/4580},
  url = {https://www.nature.com/articles/nn0199_79},
  urldate = {2022-05-01},
  abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
  issue = {1},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/marcel/Zotero/storage/3J42ZI4G/Rao and Ballard - 1999 - Predictive coding in the visual cortex a function.pdf;/Users/marcel/Zotero/storage/L5V7YGBY/nn0199_79.html}
}

@misc{rosenblattPerceptronProbabilisticModel,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization In The Brain}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, F.},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus 1 The development of this theory has been carried out at the Cornell Aeronautical Laboratory, Inc., under the sponsorship of the Office of Naval Research, Contract Nonr-2381(00). This article is primarily'an adaptation of material reported in Ref. IS, which constitutes the first full report on the program.},
  file = {/Users/marcel/Zotero/storage/PWBZ6C4L/Brain and Rosenblatt - The Perceptron A Probabilistic Model for Informat.pdf;/Users/marcel/Zotero/storage/7WNDUG9Z/download.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-05-01},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/marcel/Zotero/storage/V9Q3ANZV/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/Users/marcel/Zotero/storage/2BXPTRL8/323533a0.html}
}

@unpublished{satorrasEquivariantNormalizingFlows2021,
  title = {E(n) {{Equivariant Normalizing Flows}}},
  author = {Satorras, Victor Garcia and Hoogeboom, Emiel and Fuchs, Fabian B. and Posner, Ingmar and Welling, Max},
  date = {2021-06-08},
  eprint = {2105.09016},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  url = {http://arxiv.org/abs/2105.09016},
  urldate = {2021-07-27},
  abstract = {This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/5LKTQSVD/Satorras et al_2021_E(n) Equivariant Normalizing Flows.pdf;/Users/marcel/Zotero/storage/BEQ7KVA5/2105.html}
}

@online{saysLiesDamnLies2019,
  title = {Lies, {{Damn Lies}}, {{And TOPS}}/{{Watt}}},
  author = {Says, Olaf},
  date = {2019-01-07T20:15:00+00:00},
  url = {https://semiengineering.com/lies-damn-lies-and-tops-watt/},
  urldate = {2022-05-01},
  abstract = {Lies, Damn Lies, And TOPS/Watt Questions you need to ask to make sure you understand AI hardware performance.},
  langid = {american},
  organization = {{Semiconductor Engineering}},
  file = {/Users/marcel/Zotero/storage/F788WCIK/lies-damn-lies-and-tops-watt.html}
}

@article{senguptaGoingDeeperSpiking2019,
  title = {Going {{Deeper}} in {{Spiking Neural Networks}}: {{VGG}} and {{Residual Architectures}}},
  shorttitle = {Going {{Deeper}} in {{Spiking Neural Networks}}},
  author = {Sengupta, Abhronil and Ye, Yuting and Wang, Robert and Liu, Chiao and Roy, Kaushik},
  date = {2019},
  journaltitle = {Frontiers in Neuroscience},
  volume = {13},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2019.00095},
  urldate = {2022-05-02},
  abstract = {Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.},
  file = {/Users/marcel/Zotero/storage/4GVJZPI7/Sengupta et al. - 2019 - Going Deeper in Spiking Neural Networks VGG and R.pdf}
}

@online{shakirmMachineLearningTrick2015,
  title = {Machine {{Learning Trick}} of the {{Day}} (3): {{Hutchinson}}'s {{Trick}}},
  shorttitle = {Machine {{Learning Trick}} of the {{Day}} (3)},
  author = {{SHAKIRM}},
  date = {2015-09-13T14:35:03+00:00},
  url = {https://blog.shakirm.com/2015/09/machine-learning-trick-of-the-day-3-hutchinsons-trick/},
  urldate = {2022-05-04},
  abstract = {Hutchinson’s estimator [cite key=’hutchinson1990stochastic’]~is a simple way to obtain a stochastic estimate of the trace of a matrix. This is a simple trick that uses randomisati…},
  langid = {british},
  organization = {{The Spectator}},
  file = {/Users/marcel/Zotero/storage/BPKRDN2K/machine-learning-trick-of-the-day-3-hutchinsons-trick.html}
}

@video{simonsinstituteBiologicallyPlausibleDeep2018,
  title = {Towards {{Biologically Plausible Deep Learning}}: ...},
  shorttitle = {Towards {{Biologically Plausible Deep Learning}}},
  editor = {{Simons Institute}},
  date = {2018-04-16},
  url = {https://www.youtube.com/watch?v=d6hXF3EMv_E},
  urldate = {2022-04-19},
  abstract = {Asja Fischer, University of Bonn https://simons.berkeley.edu/talks/asj... Computational Theories of the Brain},
  editortype = {director}
}

@video{simonsinstitutePredictiveCodingModels2018,
  title = {Predictive {{Coding Models}} of {{Perception}}},
  editor = {{Simons Institute}},
  date = {2018-04-16},
  url = {https://youtu.be/P0yVuoATjzs?t=1783},
  urldate = {2022-05-02},
  abstract = {David Cox, Harvard University https://simons.berkeley.edu/talks/dav... Computational Theories of the Brain},
  editortype = {director}
}

@article{songCompetitiveHebbianLearning2000,
  title = {Competitive {{Hebbian}} Learning through Spike-Timing-Dependent Synaptic Plasticity},
  author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
  date = {2000-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {3},
  number = {9},
  pages = {919--926},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/78829},
  url = {https://www.nature.com/articles/nn0900_919},
  urldate = {2022-05-02},
  abstract = {Hebbian models of development and learning require both activity-dependent synaptic plasticity and a mechanism that induces competition between different synapses. One form of experimentally observed long-term synaptic plasticity, which we call spike-timing-dependent plasticity (STDP), depends on the relative timing of pre- and postsynaptic action potentials. In modeling studies, we find that this form of synaptic modification can automatically balance synaptic strengths to make postsynaptic firing irregular but more sensitive to presynaptic spike timing. It has been argued that neurons in vivo operate in such a balanced regime. Synapses modifiable by STDP compete for control of the timing of postsynaptic action potentials. Inputs that fire the postsynaptic neuron with short latency or that act in correlated groups are able to compete most successfully and develop strong synapses, while synapses of longer-latency or less-effective inputs are weakened.},
  issue = {9},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/marcel/Zotero/storage/QMAZ8TRE/Song et al. - 2000 - Competitive Hebbian learning through spike-timing-.pdf;/Users/marcel/Zotero/storage/DTGFD2KJ/nn0900_919.html}
}

@article{sorbaroOptimizingEnergyConsumption2020,
  title = {Optimizing the {{Energy Consumption}} of {{Spiking Neural Networks}} for {{Neuromorphic Applications}}},
  author = {Sorbaro, Martino and Liu, Qian and Bortone, Massimo and Sheik, Sadique},
  date = {2020},
  journaltitle = {Frontiers in Neuroscience},
  volume = {14},
  issn = {1662-453X},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00662},
  urldate = {2022-05-01},
  abstract = {In the last few years, spiking neural networks (SNNs) have been demonstrated to perform on par with regular convolutional neural networks. Several works have proposed methods to convert a pre-trained CNN to a Spiking CNN without a significant sacrifice of performance. We demonstrate first that quantization-aware training of CNNs leads to better accuracy in SNNs. One of the benefits of converting CNNs to spiking CNNs is to leverage the sparse computation of SNNs and consequently perform equivalent computation at a lower energy consumption. Here we propose an optimization strategy to train efficient spiking networks with lower energy consumption, while maintaining similar accuracy levels. We demonstrate results on the MNIST-DVS and CIFAR-10 datasets.},
  file = {/Users/marcel/Zotero/storage/9PVQ2XC5/Sorbaro et al. - 2020 - Optimizing the Energy Consumption of Spiking Neura.pdf}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2022-05-02},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different âthinnedâ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/Users/marcel/Zotero/storage/Q9579ZTL/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@online{StudentPrivacyPolicy,
  title = {Student Privacy Policy | {{Compliance}}},
  url = {https://compliance.admin.ox.ac.uk/student-privacy-policy#collapse968121},
  urldate = {2022-04-10}
}

@video{theuniversityofchicagoYaliAmitBiologically2012,
  title = {Yali {{Amit}} on {{Biologically Plausible Neural Networks}} for {{Invariant Visual Recognition}}},
  editor = {{The University of Chicago}},
  date = {2012-04-12},
  url = {https://www.youtube.com/watch?v=Xj_yaBaKxPA},
  urldate = {2022-04-19},
  abstract = {"Biologically Plausible Neural Networks for Invariant Visual Recognition" Yali Amit Partha Niyogi Memorial Conference: Computer Science December 5, 2011 This conference is in honor of Partha Niyogi, the Louis Block Professor in Computer Science and Statistics at the University of Chicago. Partha lost his battle with cancer in October of 2010, at the age of 43.   Partha made fundamental contributions to a variety of fields including language evolution, statistical inference, and speech recognition. The underlying themes of learning from observations and a rigorous basis for algorithms and models permeated his work.   This conference will reflect Partha's interests and pay honor to his broad vision and unique insights. The influence of his ideas and the impact on several fields will be highlighted. ➡ Subscribe: http://bit.ly/UCHICAGOytSubscribe    About \#UChicago: Since its founding in 1890, the University of Chicago has been a destination for rigorous inquiry and field-defining research. This transformative academic experience empowers students and scholars to challenge conventional thinking in pursuit of original ideas. \#UChicago on the Web: Home: http://bit.ly/UCHICAGO-homepage News: http://bit.ly/UCHICAGO-news  Facebook: http://bit.ly/UCHICAGO-FB   Twitter: http://bit.ly/UCHICAGO-TW    Instagram: http://bit.ly/UCHICAGO-IG   University of Chicago on YouTube: https://www.youtube.com/uchicago *** ACCESSIBILITY: If you experience any technical difficulties with this video or would like to make an accessibility-related request, please email digicomm@uchicago.edu.},
  editortype = {director}
}

@unpublished{thomasTensorFieldNetworks2018,
  title = {Tensor Field Networks: {{Rotation-}} and Translation-Equivariant Neural Networks for {{3D}} Point Clouds},
  shorttitle = {Tensor Field Networks},
  author = {Thomas, Nathaniel and Smidt, Tess and Kearnes, Steven and Yang, Lusann and Li, Li and Kohlhoff, Kai and Riley, Patrick},
  date = {2018-05-18},
  eprint = {1802.08219},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.08219},
  urldate = {2021-07-27},
  abstract = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marcel/Zotero/storage/67633PJV/Thomas et al_2018_Tensor field networks.pdf;/Users/marcel/Zotero/storage/2Q6UVWK7/1802.html}
}

@unpublished{toppingUnderstandingOversquashingBottlenecks2022,
  title = {Understanding Over-Squashing and Bottlenecks on Graphs via Curvature},
  author = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  date = {2022-03-16},
  eprint = {2111.14522},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.14522},
  urldate = {2022-04-04},
  abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/A58QX3KW/Topping et al. - 2022 - Understanding over-squashing and bottlenecks on gr.pdf;/Users/marcel/Zotero/storage/8JWI5C2P/2111.html}
}

@online{TorchTensorScatter,
  title = {Torch.{{Tensor}}.Scatter\_ — {{PyTorch}} 1.12 Documentation},
  url = {https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_},
  urldate = {2022-09-23},
  file = {/Users/marcel/Zotero/storage/JMJMT6ZB/torch.Tensor.scatter_.html}
}

@article{tsitourasRungeKuttaPairs2011,
  title = {Runge–{{Kutta}} Pairs of Order 5(4) Satisfying Only the First Column Simplifying Assumption},
  author = {Tsitouras, Charalampos},
  date = {2011-07-01},
  journaltitle = {Computers \& Mathematics with Applications},
  shortjournal = {Computers \& Mathematics with Applications},
  volume = {62},
  pages = {770--775},
  doi = {10.1016/j.camwa.2011.06.002},
  abstract = {Among the most popular methods for the solution of the Initial Value Problem are the Runge–Kutta pairs of orders 5 and 4. These methods can be derived solving a system of nonlinear equations for its coefficients. To achieve this, we usually admit various simplifying assumptions. The most common of them are the so-called row simplifying assumptions. Here we neglect them and present an algorithm for the construction of Runge–Kutta pairs of orders 5 and 4 based only in the first column simplifying assumption. The result is a pair that outperforms other known pairs in the bibliography when tested to the standard set of problems of DETEST. A cost free fourth order formula is also derived for handling dense output.},
  file = {/Users/marcel/Zotero/storage/3WX9KIZ5/Tsitouras - 2011 - Runge–Kutta pairs of order 5(4) satisfying only th.pdf}
}

@online{Ttumiel,
  title = {Ttumiel},
  url = {https://ttumiel.github.io/},
  urldate = {2022-05-05},
  abstract = {My personal projects.},
  file = {/Users/marcel/Zotero/storage/27PAUQJ6/replacing-bn.html}
}

@article{TutorialSETransformation,
  title = {A Tutorial on {{SE}}(3) Transformation Parameterizations and on-Manifold Optimization},
  file = {/Users/marcel/Zotero/storage/8EPDIP9N/_}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-09-22},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/9W5DA8BA/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/marcel/Zotero/storage/WEJH8LKJ/1706.html}
}

@unpublished{wagstaffUniversalApproximationFunctions2021,
  title = {Universal {{Approximation}} of {{Functions}} on {{Sets}}},
  author = {Wagstaff, Edward and Fuchs, Fabian B. and Engelcke, Martin and Osborne, Michael A. and Posner, Ingmar},
  date = {2021-07-05},
  eprint = {2107.01959},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2107.01959},
  urldate = {2021-07-27},
  abstract = {Modelling functions of sets, or equivalently, permutation-invariant functions, is a long-standing challenge in machine learning. Deep Sets is a popular method which is known to be a universal approximator for continuous set functions. We provide a theoretical analysis of Deep Sets which shows that this universal approximation property is only guaranteed if the model's latent space is sufficiently high-dimensional. If the latent space is even one dimension lower than necessary, there exist piecewise-affine functions for which Deep Sets performs no better than a na\textbackslash "ive constant baseline, as judged by worst-case error. Deep Sets may be viewed as the most efficient incarnation of the Janossy pooling paradigm. We identify this paradigm as encompassing most currently popular set-learning methods. Based on this connection, we discuss the implications of our results for set learning more broadly, and identify some open questions on the universality of Janossy pooling in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/D7WHRYXU/Wagstaff et al_2021_Universal Approximation of Functions on Sets.pdf;/Users/marcel/Zotero/storage/Q52SRY5J/2107.html}
}

@unpublished{weiler3DSteerableCNNs2018,
  title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
  shorttitle = {{{3D Steerable CNNs}}},
  author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
  date = {2018-10-27},
  eprint = {1807.02547},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.02547},
  urldate = {2021-07-27},
  abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R\^3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/IYB8IZWA/Weiler et al_2018_3D Steerable CNNs.pdf;/Users/marcel/Zotero/storage/X5RUAZUW/1807.html}
}

@unpublished{weilerCoordinateIndependentConvolutional2021,
  title = {Coordinate {{Independent Convolutional Networks}} -- {{Isometry}} and {{Gauge Equivariant Convolutions}} on {{Riemannian Manifolds}}},
  author = {Weiler, Maurice and Forré, Patrick and Verlinde, Erik and Welling, Max},
  date = {2021-06-10},
  eprint = {2106.06020},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2106.06020},
  urldate = {2021-07-27},
  abstract = {Motivated by the vast success of deep convolutional networks, there is a great interest in generalizing convolutions to non-Euclidean manifolds. A major complication in comparison to flat spaces is that it is unclear in which alignment a convolution kernel should be applied on a manifold. The underlying reason for this ambiguity is that general manifolds do not come with a canonical choice of reference frames (gauge). Kernels and features therefore have to be expressed relative to arbitrary coordinates. We argue that the particular choice of coordinatization should not affect a network's inference -- it should be coordinate independent. A simultaneous demand for coordinate independence and weight sharing is shown to result in a requirement on the network to be equivariant under local gauge transformations (changes of local reference frames). The ambiguity of reference frames depends thereby on the G-structure of the manifold, such that the necessary level of gauge equivariance is prescribed by the corresponding structure group G. Coordinate independent convolutions are proven to be equivariant w.r.t. those isometries that are symmetries of the G-structure. The resulting theory is formulated in a coordinate free fashion in terms of fiber bundles. To exemplify the design of coordinate independent convolutions, we implement a convolutional network on the M\textbackslash "obius strip. The generality of our differential geometric formulation of convolutional networks is demonstrated by an extensive literature review which explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general surfaces as specific instances of coordinate independent convolutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/QMKYGJ4Q/Weiler et al_2021_Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant.pdf;/Users/marcel/Zotero/storage/9GS3MU3A/2106.html}
}

@unpublished{weilerGeneralEquivariantSteerable2021,
  title = {General \${{E}}(2)\$-{{Equivariant Steerable CNNs}}},
  author = {Weiler, Maurice and Cesa, Gabriele},
  date = {2021-04-06},
  eprint = {1911.08251},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1911.08251},
  urldate = {2021-07-30},
  abstract = {The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of \$E(2)\$-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group \$E(2)\$ and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. \$E(2)\$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marcel/Zotero/storage/UDUQF6VK/Weiler_Cesa_2021_General $E(2)$-Equivariant Steerable CNNs.pdf;/Users/marcel/Zotero/storage/9J9H6Q77/1911.html}
}

@article{wuAIPhysicistUnsupervised2019,
  title = {Toward an {{AI Physicist}} for {{Unsupervised Learning}}},
  author = {Wu, Tailin and Tegmark, Max},
  date = {2019-09-19},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {100},
  number = {3},
  eprint = {1810.10525},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics},
  pages = {033311},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.100.033311},
  url = {http://arxiv.org/abs/1810.10525},
  urldate = {2022-08-15},
  abstract = {We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's razor, unification and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and "snap" learned theories into simple symbolic formulas. Theories are stored in a "theory hub", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the toy "AI Physicist" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Physics - Computational Physics},
  file = {/Users/marcel/Zotero/storage/QEAZDLZE/Wu and Tegmark - 2019 - Toward an AI Physicist for Unsupervised Learning.pdf;/Users/marcel/Zotero/storage/IRER7CAQ/1810.html}
}

@misc{wuHighresolutionNovoStructure2022,
  title = {High-Resolution de Novo Structure Prediction from Primary Sequence},
  author = {Wu, Ruidong and Ding, Fan and Wang, Rui and Shen, Rui and Zhang, Xiwen and Luo, Shitong and Su, Chenpeng and Wu, Zuofan and Xie, Qi and Berger, Bonnie and Ma, Jianzhu and Peng, Jian},
  date = {2022-07-22},
  pages = {2022.07.21.500999},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.07.21.500999},
  url = {https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1},
  urldate = {2022-08-01},
  abstract = {Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.},
  langid = {english},
  file = {/Users/marcel/Zotero/storage/UY43NL36/Wu et al. - 2022 - High-resolution de novo structure prediction from .pdf;/Users/marcel/Zotero/storage/UFE4KIYV/2022.07.21.html}
}

@unpublished{wuPointConvDeepConvolutional2020,
  title = {{{PointConv}}: {{Deep Convolutional Networks}} on {{3D Point Clouds}}},
  shorttitle = {{{PointConv}}},
  author = {Wu, Wenxuan and Qi, Zhongang and Fuxin, Li},
  date = {2020-11-09},
  eprint = {1811.07246},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.07246},
  urldate = {2021-07-27},
  abstract = {Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and density functions through kernel density estimation. The most important contribution of this work is a novel reformulation proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcel/Zotero/storage/BBFGKKLW/Wu et al_2020_PointConv.pdf;/Users/marcel/Zotero/storage/UNBSHQUK/1811.html}
}

@unpublished{yangPointFlow3DPoint2019,
  title = {{{PointFlow}}: {{3D Point Cloud Generation}} with {{Continuous Normalizing Flows}}},
  shorttitle = {{{PointFlow}}},
  author = {Yang, Guandao and Huang, Xun and Hao, Zekun and Liu, Ming-Yu and Belongie, Serge and Hariharan, Bharath},
  date = {2019-09-02},
  eprint = {1906.12320},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.12320},
  urldate = {2021-07-27},
  abstract = {As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code will be available at https://github.com/stevenygd/PointFlow.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/3AQELNM5/Yang et al_2019_PointFlow.pdf;/Users/marcel/Zotero/storage/HIRHDBD9/1906.html}
}

@video{yannickilcherBackpropagationBrain2020,
  title = {Backpropagation and the Brain},
  editor = {{Yannic Kilcher}},
  date = {2020-04-20},
  url = {https://www.youtube.com/watch?v=a0f07M2uj_A},
  urldate = {2022-04-19},
  abstract = {Geoffrey Hinton and his co-authors describe a biologically plausible variant of backpropagation and report evidence that such an algorithm might be responsible for learning in the brain. https://www.nature.com/articles/s4158... Abstract: During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain. Authors: Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman \& Geoffrey Hinton Links: YouTube: https://www.youtube.com/c/yannickilcher Twitter: https://twitter.com/ykilcher BitChute: https://www.bitchute.com/channel/yann... Minds: https://www.minds.com/ykilcher},
  editortype = {director}
}

@video{yannickilcherNeurallyPlausibleModel2019,
  title = {A Neurally Plausible Model Learns Successor Representations in Partially Observable Environments},
  editor = {{Yannic Kilcher}},
  date = {2019-11-07},
  url = {https://www.youtube.com/watch?v=KXEEqcwXn8w},
  urldate = {2022-04-19},
  abstract = {Successor representations are a mid-point between model-based and model-free reinforcement learning. This paper learns successor representation in environments where only incomplete information is available. Abstract: Animals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible. Authors: Eszter Vertes, Maneesh Sahani Links: YouTube: https://www.youtube.com/c/yannickilcher Twitter: https://twitter.com/ykilcher BitChute: https://www.bitchute.com/channel/yann... Minds: https://www.minds.com/ykilcher},
  editortype = {director}
}

@unpublished{zahnPruningDeepNeural2022,
  title = {Pruning Deep Neural Networks Generates a Sparse, Bio-Inspired Nonlinear Controller for Insect Flight},
  author = {Zahn, Olivia and Bustamante Jr., Jorge and Switzer, Callin and Daniel, Thomas and Kutz, J. Nathan},
  date = {2022-01-05},
  eprint = {2201.01852},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  url = {http://arxiv.org/abs/2201.01852},
  urldate = {2022-05-02},
  abstract = {Insect flight is a strongly nonlinear and actuated dynamical system. As such, strategies for understanding its control have typically relied on either model-based methods or linearizations thereof. Here we develop a framework that combines model predictive control on an established flight dynamics model and deep neural networks (DNN) to create an efficient method for solving the inverse problem of flight control. We turn to natural systems for inspiration since they inherently demonstrate network pruning with the consequence of yielding more efficient networks for a specific set of tasks. This bio-inspired approach allows us to leverage network pruning to optimally sparsify a DNN architecture in order to perform flight tasks with as few neural connections as possible, however, there are limits to sparsification. Specifically, as the number of connections falls below a critical threshold, flight performance drops considerably. We develop sparsification paradigms and explore their limits for control tasks. Monte Carlo simulations also quantify the statistical distribution of network weights during pruning given initial random weights of the DNNs. We demonstrate that on average, the network can be pruned to retain approximately 7\% of the original network weights, with statistical distributions quantified at each layer of the network. Overall, this work shows that sparsely connected DNNs are capable of predicting the forces required to follow flight trajectories. Additionally, sparsification has sharp performance limits.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Quantitative Methods},
  file = {/Users/marcel/Zotero/storage/UIJWBB6Q/Zahn et al. - 2022 - Pruning deep neural networks generates a sparse, b.pdf;/Users/marcel/Zotero/storage/S9WVR8ZR/2201.html}
}

@inproceedings{zhaoEnergyEfficientComputinginMemoryNeuromorphic2019,
  title = {An {{Energy-Efficient Computing-in-Memory Neuromorphic System}} with {{On-Chip Training}}},
  booktitle = {2019 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  author = {Zhao, Zhao and Wang, Yuan and Zhang, Xinyue and Cui, Xiaoxin and Huang, Ru},
  date = {2019-10},
  pages = {1--4},
  issn = {2163-4025},
  doi = {10.1109/BIOCAS.2019.8918995},
  abstract = {The aim of neuromorphic computing system is to implement the computational power and efficiency of the human brain. Computing-in-memory (CIM) is a promising and energy-efficient way to perform intensive computations, whose structure is similar to human brain synapse. A 8.78TOPS/W biologically-inspired neuromorphic computing system for pattern recognition based on CIM architecture is presented in this work. The proposed system supports on-chip training with energy-efficient bio-plausible spike-timing-dependent plasticity (STDP) rule and performs multiply-and-accumulate (MAC) computations inside SRAM array during inference, which greatly reduces the energy consumption. Simulated in 65-nm technology, the proposed system achieves good performance and energy efficiency for pattern recognition. The total energy consumption of training and classifying per image of the proposed system is 0.20 nJ. And the proposed spiking neural network (SNN) just consumes 0.074mW at 1.0V with the throughput of 2.5M images/s in inference phase.},
  eventtitle = {2019 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  keywords = {analog computation,bio-plasticity synapses,computing-in-memory (CIM),multiply-and-accumulate (MAC),neuromorphic computing system,Neuromorphics,Neurons,on-chip training,Random access memory,Static VAr compensators,Synapses,Testing,Training},
  file = {/Users/marcel/Zotero/storage/KCMDLCR9/Zhao et al. - 2019 - An Energy-Efficient Computing-in-Memory Neuromorph.pdf;/Users/marcel/Zotero/storage/S63JWXIW/8918995.html}
}

@unpublished{zhaoQuaternionEquivariantCapsule2020,
  title = {Quaternion {{Equivariant Capsule Networks}} for {{3D Point Clouds}}},
  author = {Zhao, Yongheng and Birdal, Tolga and Lenssen, Jan Eric and Menegatti, Emanuele and Guibas, Leonidas and Tombari, Federico},
  date = {2020-08-23},
  eprint = {1912.12098},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.12098},
  urldate = {2021-07-27},
  abstract = {We present a 3D capsule module for processing point clouds that is equivariant to 3D rotations and translations, as well as invariant to permutations of the input points. The operator receives a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end transformation equivariance through a novel dynamic routing procedure on quaternions. Further, we theoretically connect dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving \textbackslash emph\{iterative re-weighted least squares\} (IRLS) problems with provable convergence properties. It is shown that such group dynamic routing can be interpreted as robust IRLS rotation averaging on capsule votes, where information is routed based on the final inlier scores. Based on our operator, we build a capsule network that disentangles geometry from pose, paving the way for more informative descriptors and a structured latent space. Our architecture allows joint object classification and orientation estimation without explicit supervision of rotations. We validate our algorithm empirically on common benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/marcel/Zotero/storage/A2ANEAHN/Zhao et al_2020_Quaternion Equivariant Capsule Networks for 3D Point Clouds.pdf;/Users/marcel/Zotero/storage/ICTM8GKP/1912.html}
}

@unpublished{zhuCorrespondenceFreePointCloud2021,
  title = {Correspondence-{{Free Point Cloud Registration}} with {{SO}}(3)-{{Equivariant Implicit Shape Representations}}},
  author = {Zhu, Minghan and Ghaffari, Maani and Peng, Huei},
  date = {2021-07-21},
  eprint = {2107.10296},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.10296},
  urldate = {2021-07-30},
  abstract = {This paper proposes a correspondence-free method for point cloud rotational registration. We learn an embedding for each point cloud in a feature space that preserves the SO(3)-equivariance property, enabled by recent developments in equivariant neural networks. The proposed shape registration method achieves three major advantages through combining equivariant feature learning with implicit shape models. First, the necessity of data association is removed because of the permutation-invariant property in network architectures similar to PointNet. Second, the registration in feature space can be solved in closed-form using Horn's method due to the SO(3)-equivariance property. Third, the registration is robust to noise in the point cloud because of implicit shape learning. The experimental results show superior performance compared with existing correspondence-free deep registration methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcel/Zotero/storage/6C52UFGZ/Zhu et al_2021_Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit.pdf;/Users/marcel/Zotero/storage/H2HMIDZP/2107.html}
}

@online{ZoteroYourPersonal,
  title = {Zotero | {{Your}} Personal Research Assistant},
  url = {https://www.zotero.org/start},
  urldate = {2022-03-17},
  file = {/Users/marcel/Zotero/storage/MM2CFYVV/start.html}
}


